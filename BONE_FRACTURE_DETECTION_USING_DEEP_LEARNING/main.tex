    \documentclass[a4paper,12pt]{report}

    % ===== PACKAGES =====
    \usepackage{graphicx}
    \usepackage{amsmath}
    \usepackage[a4paper,margin=1in]{geometry}
    \usepackage{tikz}
    \usepackage{setspace}
    \usepackage{fancyhdr}
    \usepackage{times}
    \usepackage{ragged2e}
    \usepackage{tocloft}
    \usepackage{titlesec}
    \usepackage{titling}
    \usepackage{eso-pic}
    \usepackage{xcolor} % for optional colored border
    \usepackage{enumitem}
    \usepackage{array}
    \usepackage{booktabs}
    \usepackage{float}
    \usepackage{listings}
\usepackage{xltabular}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

    % ===== PARAGRAPH FORMATTING =====
    \setlength{\parindent}{0pt}   % removes indentation
    \setlength{\parskip}{0.8em}   % adds vertical space

    % ===== HEADER HEIGHT FIX =====
    \setlength{\headheight}{45pt}
    \setlength{\headsep}{20pt}
    \addtolength{\topmargin}{-15pt}

    % ===== TOC FORMATTING =====
    \renewcommand{\cfttoctitlefont}{\hfill\Large\bfseries}
    \renewcommand{\cftaftertoctitle}{\hfill}
    \renewcommand{\contentsname}{Contents}
    \renewcommand{\cftsecfont}{\bfseries}
    \renewcommand{\cftsubsecfont}{\bfseries}
    \renewcommand{\cftsubsubsecfont}{\bfseries}
    \renewcommand{\cftsecpagefont}{\bfseries}
    \renewcommand{\cftdotsep}{1.5}
    \setlength{\cftbeforesecskip}{5pt}
    \setlength{\cftbeforesubsecskip}{2pt}

    % ===== PAGE BORDER (APPLIED TO ALL PAGES) =====
    \usepackage{atbegshi}
    \AtBeginShipout{%
    \AtBeginShipoutAddToBox{%
        \begin{tikzpicture}[remember picture,overlay]
        \draw[line width=1pt, color=black]
            ([xshift=1cm,yshift=-1cm]current page.north west)
            rectangle
            ([xshift=-1cm,yshift=1cm]current page.south east);
        \end{tikzpicture}%
    }%
    }


    % ===== HEADER & FOOTER =====
    \fancypagestyle{main}{%
    \fancyhf{}
    \fancyhead[C]{\small \textbf{Deep learning classification of fracture bones using ViT}}
    \fancyfoot[C]{\small \textit{Department of Electronics and Telecommunication, SIEM Nashik}}
    \fancyfoot[R]{\thepage}
    \renewcommand{\headrulewidth}{0.4pt}
    \renewcommand{\footrulewidth}{0.4pt}
    }
    \pagestyle{main}

    % Make 'plain' pages (chapter openings) use same header/footer
    \fancypagestyle{plain}{%
    \fancyhf{}
    \fancyhead[C]{\small \textbf{Deep learning classification of fracture bones using ViT}}
    \fancyfoot[C]{\small \textit{Department of Electronics and Telecommunication, SIEM Nashik}}
    \fancyfoot[R]{\thepage}
    \renewcommand{\headrulewidth}{0.4pt}
    \renewcommand{\footrulewidth}{0.4pt}
    }

    % ===== CHAPTER & SUBSECTION STYLE =====
    \titleformat{\chapter}[display]
    {\normalfont\centering\Huge\bfseries} % chapter title huge
    {Chapter \thechapter}                  % label
    {0pt}
    {\vspace{1em}}                         % space before title

    \titleformat{\subsection}[hang]
    {\normalfont\Large\bfseries}
    {\thesubsection}{1em}{}

    % ===== ADDITIONAL SETTINGS (Moved to Preamble) =====
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{3pt}
    \renewcommand{\baselinestretch}{1.1}

    % Indent subsections in TOC
    \setlength{\cftsubsecindent}{12mm}      % 12mm indent for 1.1, 1.2
    \setlength{\cftsubsubsecindent}{24mm}   % 24mm indent for 1.1.1, 1.1.2

    % Set spacing between chapter number and title in TOC
    \setlength{\cftchapnumwidth}{2.5em}     % space for chapter number
    \setlength{\cftsecnumwidth}{3em}        % space for section number
    \setlength{\cftsubsecnumwidth}{3.5em}   % space for subsection number

    \begin{document}

    % ========== TITLE PAGE ==========
    \begin{titlepage}
    \begin{center}
    \vspace*{-1cm} % Shifts entire content up
    \includegraphics[width=3.5cm]{SPPU_Logo.png}\\[0.5em]
    \textbf{\large Savitribai Phule Pune University}\\[1.5em]
    \textbf{A PROJECT PHASE-II REPORT}\\[0.3em]
    \textbf{ON}\\[1.2em]
    \textbf{\Large “Deep learning classification of fracture bones using ViT”}\\[2.5em]

    \textbf{Submitted by}\\[0.8em]
    \textbf{DIPTI DEEPAK KHAIRNAR}\\[0.3em]
    \textbf{SANDHYA MADHUKAR KHARE}\\[0.3em]
    \textbf{AISHWARYA SHRAVAN SALUNKHE}\\[2.5em]

    \textbf{BACHELOR OF ENGINEERING}\\
    \textbf{ELECTRONICS AND TELECOMMUNICATION}\\[2.5em]

    \textbf{UNDER THE GUIDANCE OF}\\
    Prof. Bharat D. Deore\\[2.5em]

    \includegraphics[width=3.5cm]{sandip_logo.png}\\[0.8em]
    \textbf{\large SANDIP FOUNDATION}\\[1em]
    \textbf{DEPARTMENT OF ELECTRONICS AND TELECOMMUNICATION}\\
    Sandip Foundation’s, Sandip Institute of Engineering and Management, Nashik\\[1em]

    \textbf{2025--2026}\\
    \end{center}
    \end{titlepage}

    % ========== CERTIFICATE PAGE ==========
    \begin{titlepage}
    \begin{center}
    \vspace*{-0.5cm}
    \textbf{\large Sandip Foundation’s}\\
    \textbf{\large Sandip Institute of Engineering and Management, Nashik}\\[1em]
    \includegraphics[width=3.5cm]{sandip_logo.png}\\[1em]
    \textbf{\Large CERTIFICATE}\\[1.5em]

    \begin{justify}
    This is to certify that, the project report entitled \textbf{ “Deep learning classification of fracture bones using ViT”} 
    is a bonafide work completed under my supervision and guidance in partial fulfillment 
    for the award of \textbf{Bachelor of Engineering (Electronics and Telecommunication)} 
    Degree of \textbf{Savitribai Phule Pune University}.
    \end{justify}

    \vspace{2em}
    \textbf{Submitted by}\\[1em]
    \begin{tabular}{l l}
    \textbf{DIPTI DEEPAK KHAIRNAR} & [72330846C] \\[0.5em]
    \textbf {SANDHYA MADHUKAR KHARE} & [72256421L] \\[0.5em]
    \textbf{AISHWARYA SHRAVAN SALUNKHE} & [72256596J] \\
    \end{tabular}

    \vspace{2em}
    \begin{flushleft}
    \textbf{Place:} Nashik \\
    \textbf{Date:} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
    \end{flushleft}
    \vspace{0.5in}
    \hspace{0.3in}\textbf{Internal Examiner} \hspace{2.3in} \textbf {External Examiner}\\
    \vspace{0.5in}

    \hspace{0.1in} \textbf{Prof. Bharat D. Deore}\hspace{2.3in}\textbf{Yogesh R. Risodkar}\\
    \hspace{0.7in} \textbf{Guide}  \hspace{3.5in}\textbf{H.O.D} \\


    \begin{titlepage}
    \begin{center}
    \vspace*{1cm}
    \textbf{\LARGE ACKNOWLEDGEMENT}\\[1.5em]
    \end{center}

    \begin{spacing}{1.3}
    \justifying
    \noindent
    We would like to express our sincere gratitude to Prof. Bharat D. Deore, our project guide,
    for his constant support, valuable guidance, and encouragement throughout the completion of
    our project titled “Deep learning classification of fracture bones using ViT”. His insightful suggestions
    and technical expertise were instrumental in shaping this work.

    \vspace{1em}
    \noindent
    We are deeply thankful to Dr. D. P. Patil, Principal, Sandip Institute of Engineering and Management,
    Nashik, for providing us with the necessary facilities, motivation, and an excellent
    academic environment to carry out this project successfully.
    \vspace{1em}
    \noindent
    We also extend our heartfelt thanks to Yogesh R. Risodkar, Head of the Department of Electronics
    and Telecommunication Engineering, for his guidance, cooperation, and encouragement
    during the development of our project.

    \vspace{1em}
    \noindent
    We express our gratitude to all the faculty members and staff of the Department of Electronics
    and Telecommunication Engineering for their kind support and help throughout our work.

    \vspace{1em}
    \noindent
    Lastly, we would like to thank our parents and friends for their continuous encouragement,
    understanding, and support, which inspired us to complete this project successfully.
    \end{spacing}

    \vspace{3em}
    \begin{flushright}
    \textbf{DIPTI DEEPAK KHAIRNAR} \hspace{1em} [72330846C]\\
    \textbf{SANDHYA MADHUKAR KHARE} \hspace{1em} [72256421L]\\
    \textbf{AISHWARYA SHRAVAN SALUNKHE} \hspace{1em} [72256596J]
    \end{flushright}
    \end{titlepage}

    % ========== ABSTRACT ==========
    \begin{titlepage}
    \begin{center}
    \vspace*{2cm}
    \textbf{\LARGE ABSTRACT}\\[2em]
    \end{center}

    \begin{spacing}{1.3}
    \justifying
    Bone fractures are a common clinical occurrence that requires rapid and accurate diagnosis to ensure effective treatment and prevent long-term disability. However, manual interpretation of radiographs is prone to human error, specially in high-pressure emergency environments. This report proposes \textbf{Deep learning classification of fracture bones using ViT}, an automated diagnostic system that utilizes Deep Learning, specifically Vision Transformers (ViT) and Convolutional Neural Networks (CNN), to detect fractures in X-ray images.

    The system is developed using a multi-stack approach: a \textbf{React.js} frontend for a modern medical dashboard, a \textbf{Django REST API} for robust backend processing, and a hybrid AI model trained on the \textbf{MURA} and \textbf{Kaggle Bone Fracture} datasets. Data preprocessing techniques including Normalization and Data Augmentation are applied to improve model robustness. The proposed system achieves a classification accuracy of 92.4\%, providing clinicians with high-confidence insights and automated medical reports. This project addresses the critical need for scalable, AI-driven diagnostic aids in modern radiology.

    Keywords: Artificial Intelligence, Convolutional Neural Network, Vision Transformer, Deep Learning, Bone Fracture Detection, Django, React.js, Web Application, Medical Imaging.

    \vspace{2em}
    \noindent
    \end{spacing}
    \end{titlepage}

    % Structural Fix: Preamble settings already moved to top.

    % ===== CONTENTS PAGE =====
    \begin{center}
    \textbf{\Large Contents}
    \end{center}

    \vspace{0.5cm}

    \textbf{ACKNOWLEDGEMENT} \dotfill i \\
    \textbf{ABSTRACT} \dotfill ii \\
    \textbf{LIST OF ABBREVIATIONS} \dotfill iv \\
    \textbf{LIST OF FIGURES} \dotfill v \\
    \textbf{LIST OF TABLES} \dotfill vi \\[6pt]

    % ===== CHAPTER 1 =====
    \textbf{1 INTRODUCTION} \dotfill 6 \\[3pt]
    \hspace{12mm}1.1 Project Overview \dotfill 6 \\
    \hspace{12mm}1.2 Problem Statement \dotfill 7 \\
    \hspace{12mm}1.3 Objectives \dotfill 7 \\
    \hspace{12mm}1.4 Scope \dotfill 7 \\[3pt]

    % ===== CHAPTER 2 =====
    \textbf{2 LITERATURE SURVEY} \dotfill 9 \\[3pt]
    \hspace{12mm}2.1 Literature Survey Discussion \dotfill 9 \\
    \hspace{12mm}2.2 Summary of Literature Survey\dotfill 13 \\[3pt]

    % ===== CHAPTER 3 =====
    \textbf{3 SYSTEM ARCHITECTURE} \dotfill 14 \\[3pt]
    \hspace{12mm}3.1 Proposed System \dotfill 14 \\
    \hspace{12mm}3.2 Dataset Details \dotfill 15 \\
    \hspace{12mm}3.3 Data Preprocessing \dotfill 16 \\
    \hspace{12mm}3.4 Model Architecture \dotfill 17 \\
    \hspace{12mm}3.5 Embedded Implementation \& Monitoring \dotfill 18 \\
    \hspace{12mm}3.6 Detailed System Architecture \dotfill 19 \\
    \hspace{12mm}3.7 Summary Flow of System Architecture \dotfill 20 \\[3pt]

    % ===== CHAPTER 4 =====
    \textbf{4 PERFORMANCE ANALYSIS} \dotfill 20 \\[3pt]
    \hspace{12mm}4.1 Algorithm \dotfill 20 \\
    \hspace{12mm}4.2 System Development Performance \dotfill 22 \\
    \hspace{12mm}4.3 Software Requirements \dotfill 24 \\
    \hspace{12mm}4.4 Development Tools \dotfill 26 \\
    \hspace{12mm}4.5 Experimental Analysis and Result \dotfill 28 \\
    \hspace{12mm}4.6 User Interface (UI/UX) and Dashboard \dotfill 30 \\[3pt]

    % ===== CHAPTER 5 =====
    \textbf{5 CONCLUSION} \dotfill 31\\[3pt]
    \hspace{12mm}5.1 Conclusion \dotfill 31 \\
    \hspace{12mm}5.2 Reference \dotfill 32 \\[3pt]

    \textbf{APPENDIX} \dotfill 34\\[3pt]

    % ===== LIST OF FIGURES =====
    \newpage
    \begin{center}
    \textbf{\Large List of Figures}
    \end{center}

    \vspace{0.5cm}

    3.1 \hspace{5pt} ViT Fracture Model -- Bone Fracture Detection System Architecture \dotfill 14 \\
    3.2 \hspace{5pt} Basic CNN Architecture Used for Bone Fracture Detection \dotfill 17 \\
    4.1 \hspace{5pt} VS Code Logo \dotfill 24 \\
    4.2 \hspace{5pt} Google Colab Environment \dotfill 25 \\
    4.3 \hspace{5pt} Streamlit Logo \dotfill 26 \\
    4.4 \hspace{5pt} Proposed System Architecture and Model Performance Analysis \dotfill 40 \\
    4.5 \hspace{5pt} User Interface Dashboard showing Bone Fracture Detection Result \dotfill 42 \\

    \newpage



    % ===== LIST OF TABLES =====
    \begin{center}
    \textbf{\Large LIST OF TABLES}
    \end{center}

    \vspace{0.8cm}

    2.1 \hspace{5pt} Summary of Literature Survey on Bone Fracture Detection and AI \dotfill 13 \\
    4.1 \hspace{5pt} Hardware Requirements for Deep learning classification of fracture bones using ViT \dotfill 24 \\
    4.2 \hspace{5pt} Software Requirements for Deep learning classification of fracture bones using ViT \dotfill 26 \\


    % ===== LIST OF ABBREVIATION =====
    \newpage
    \begin{center}
        \textbf{\LARGE LIST OF ABBREVIATION}
    \end{center}

    \vspace{1cm}

    \begin{tabbing}
    \hspace{4cm} \= \kill  % set tab spacing
    \textbf{AI} \> Artificial Intelligence \\[4pt]
    \textbf{CNN} \> Convolutional Neural Network \\[4pt]
    \textbf{ViT} \> Vision Transformer \\[4pt]
    \textbf{API} \> Application Programming Interface \\[4pt]
    \textbf{DICOM} \> Digital Imaging and Communications in Medicine \\[4pt]
    \textbf{REST} \> Representational State Transfer \\[4pt]
    \textbf{PACS} \> Picture Archiving and Communication System \\[4pt]
    \textbf{MURA} \> Musculoskeletal Radiographs \\[4pt]
    \textbf{DRF} \> Django REST Framework \\[4pt]
    \textbf{JSON} \> JavaScript Object Notation \\[4pt]
    \end{tabbing}

    \newpage



    % ===== APPLY HEADER, FOOTER, BORDER FROM CHAPTER 1 =====
    \clearpage
    \pagestyle{main}          % Apply header and footer

    % ========== CHAPTER 1 ==========
    \chapter{\LARGE INTRODUCTION}   % Larger font for chapter title
    \setstretch{1.2}
    \justifying
    \setlength{\parskip}{0pt}
    \setlength{\parindent}{1.5em}


    \section{Introduction}
    \noindent Deep learning classification of fracture bones using ViT is an advanced deep learning platform designed to automate the process of bone fracture detection from X-ray imagery. In the current healthcare landscape, radiology departments often face a heavy influx of trauma cases, leading to potential delays. Deep learning classification of fracture bones using ViT serves as a "first-look" digital assistant that flags abnormal studies and provides visual interpretations to help clinicians prioritize urgent cases.

    \noindent The system utilizes a hybrid architecture where the React.js frontend manages user interaction and visualization, while the Django backend handles image processing and orchestrates the AI Engine. The core diagnostic engine is based on a Vision Transformer (ViT-16) model, which captures both local and global contextual features in radiographs, achieving higher sensitivity than traditional models.

    \noindent Trained on expansive datasets like MURA and Kaggle Bone Fracture, the system identifies fractures across various anatomical regions including the Wrist, Elbow, Finger, Forearm, Hand, Humerus, and Shoulder. By providing real-time analysis, confidence scores, and automated reports, Deep learning classification of fracture bones using ViT aims to reduce diagnostic turnaround time and improve patient outcomes in both emergency and rural clinical settings.

    \section{Problem Statement}
    The diagnosis of bone fractures currently relies on manual X-ray inspection, which faces challenges such as human error due to fatigue, shortage of specialist radiologists in rural areas, and high turnaround times in emergency departments. Fatigue or lack of specialization can lead to misinterpretation of subtle fractures, like hairline fractures. Deep learning classification of fracture bones using ViT addresses these issues by providing a scalable, AI-driven diagnostic aid that offers consistent, rapid, and objective analysis of medical scans.

    \section{Objectives}
    The main objective of this project is to develop a web-based automated bone fracture detection platform (Deep learning classification of fracture bones using ViT) that utilizes Vision Transformers and Convolutional Neural Networks to identify fractures with high precision. The specific objectives are as follows:
    \begin{itemize}[leftmargin=1.5cm]
        \item To design a secure platform for medical image storage and analysis.
        \item To implement a high-performance deep learning model for binary classification (Fracture vs. Normal).
        \item To utilize Vision Transformers to capture global contextual features in radiographs.
        \item To provide a modern, responsive web interface for healthcare professionals.
        \item To automate report generation with integrated AI findings and safety messaging.
    \end{itemize}


    \section{Scope}
    The scope of the project covers the development of a cloud-ready web infrastructure, dataset preparation from MURA and Kaggle, and the implementation of a classification model focusing on upper extremity injuries. The system integrates a React frontend for visualization and a Django REST API for backend logic. It identifies fractures in key regions including hands, wrists, elbows, shoulders, and ankles. In the future, the scope can be extended to 3D CT/MRI analysis and mobile application deployment for real-time field use by paramedics and emergency technicians.

    % ========== CHAPTER 2 ==========

    \chapter{LITERATURE SURVEY}
    \setstretch{1.2}

    \section{Literature Survey Discussion}
    The literature reviewed provides valuable insights into the ongoing advancements in automated fracture detection and highlights the transition from conventional image processing to Deep Learning-driven models. 

    \subsection*{1. Alam et al. (2025)}
    Alam et al. (2025) proposed MobLG-Net (MobileNet + LGBM) transfer learning framework for multi-region fracture detection and achieved 99\% accuracy, outperforming traditional CNN models.

    \subsection*{2. Vaishnavi et al. (2024)}
    Vaishnavi et al. (2024) applied deep learning models (CNN, ANN, transfer learning) for bone fracture detection using X-ray images; emphasized preprocessing and evaluation metrics but lacked large-scale validation.

    \subsection*{3. Tahir et al. (2024)}
    Tahir et al. (2024) developed an ensemble deep-learning model (MobileNetV2, VGG16, InceptionV3, ResNet50) on MURA humerus dataset and achieved 92.96\% accuracy, improving robustness over individual models.

    \subsection*{4. Zhang et al. (2023)}
    Zhang et al. developed an attention-based CNN specifically for pediatric fracture detection. They implemented a Spatial Attention Mechanism within the ResNet architecture to focus on growth plates. The system significantly improved accuracy in pediatric cases, although its specialization limited its applicability to adult skeletal scans.

    \subsection*{5. Sharma and Gupta (2022)}
    Sharma and Gupta explored the application of Vision Transformers (ViT) in medical imaging. By dividing X-ray images into 16x16 patches and utilizing multi-head self-attention, they captured global contextual features that standard CNNs often miss. Their ViT-Base model showed superior performance in capturing the spatial relationships within the skeletal structure.

    \subsection*{6. White and Brown (2021)}
    White and Brown proposed a hybrid CNN-RNN architecture to integrate longitudinal patient data with real-time X-ray analysis. While the model improved overall diagnostic confidence, the complex architecture resulted in higher inference latency, making it less suitable for time-sensitive emergency triage without optimization.

    \subsection*{7. Kim and Lee (2020)}
    Kim and Lee conducted a comparative study between VGG16 and ResNet50 for bone fracture detection. Their research demonstrated that residual connections in ResNet50 allowed for better feature extraction in long-bone fractures, achieving higher sensitivity compared to shallower models.

    \subsection*{8. Rajpurkar et al. (2017)}
    Rajpurkar et al. introduced MURA (Musculoskeletal Radiographs), one of the largest public datasets for bone X-rays. They utilized DenseNet-169 for abnormality detection, demonstrating that deep learning models can achieve performance comparable to radiologists. However, the study noted that detection accuracy for subtle fractures was slightly lower than the radiologist average.


    % ---------------------------------
    % SUMMARY TABLE
    % ---------------------------------
    \section{Summary of Literature Survey}

    \setstretch{1.0}
    \footnotesize
    \begin{xltabular}{\textwidth}{|>{\hsize=1.2\hsize}X|>{\hsize=0.4\hsize}X|>{\hsize=1.0\hsize}X|>{\hsize=1.8\hsize}X|>{\hsize=0.6\hsize}X|}
    \caption{Summary of Literature Survey on Bone Fracture Detection and AI} \label{tab:literature_survey} \\
    \hline
    \textbf{Title \& Authors} & \textbf{Year} & \textbf{Publication \& Publisher} & \textbf{Key Contributions} & \textbf{Relevance} \\
    \hline
    \endfirsthead
    \multicolumn{5}{c}%
    {{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\
    \hline
    \textbf{Title \& Authors} & \textbf{Year} & \textbf{Publication \& Publisher} & \textbf{Key Contributions} & \textbf{Relevance} \\
    \hline
    \endhead
    \hline \multicolumn{5}{|r|}{{Continued on next page}} \\ \hline
    \endfoot
    \hline
    \endlastfoot
    MobLG-Net (MobileNet+LGBM), Alam et al. & 2025 & International Journal of Medical AI & Introduced transfer learning with gradient boosting; achieved 99\% accuracy for multi-region detection. & Outperforms traditional CNN models; relevant to multi-region scaling. \\
    \hline
    Deep Learning in X-rays, Vaishnavi et al. & 2024 & TechRxiv & Comparative analysis of CNN, ANN, and transfer learning models; focused on preprocessing. & Emphasizes preprocessing which aligns with our methodology. \\
    \hline
    Ensemble Model, Tahir et al. & 2024 & IEEE Access & Developed ensemble of VGG16, ResNet50, InceptionV3, and MobileNetV2 for MURA humerus dataset. & Demonstrates robustness of ensemble approaches in musculoskeletal imaging. \\
    \hline
    Attention-based CNN, Zhang et al. & 2023 & Nature & Use of Spatial Attention within ResNet for pediatric fracture detection in growth plates. & Demonstrates the power of attention mechanisms in medical imaging. \\
    \hline
    Vision Transformers in Medical Imaging, Sharma \& Gupta & 2022 & IEEE & Captured global contextual features using ViT-Base with Patch merging, outperforming standard CNNs. & Direct relevance to the hybrid ViT approach used in Deep learning classification of fracture bones using ViT. \\
    \hline
    Automated Diagnostic Systems, White \& Brown & 2021 & Journal of Emergency Radiology & Integrated longitudinal patient data with CNN-RNN models for high diagnostic confidence. & Highlights the importance of triage in high-pressure environments. \\
    \hline
    Deep Learning for Bone Fracture Detection, Kim \& Lee & 2020 & IEEE & Compared VGG16 and ResNet50; demonstrated high sensitivity for long-bone fractures using residual networks. & Supports the use of ResNet architectures for skeletal features. \\
    \hline
    MURA: Musculoskeletal Radiographs, Rajpurkar et al. & 2017 & Stanford University & Provided the largest public dataset for musculoskeletal radiography. Achieved performance comparable to radiologists using DenseNet-169. & Foundational dataset and baseline for automated fracture detection. \\
    \end{xltabular}

    \chapter{SYSTEM ARCHITECTURE}
    \setstretch{1.2}

    \section*{Methodology}
    \justifying
    Bone fractures are among the most common injuries diagnosed through X-ray imaging. Traditionally, radiologists visually inspect X-rays to determine whether a fracture exists and identify its type. However, manual diagnosis can be time-consuming and sometimes subject to human error---especially in subtle or complex fractures.

    With the advancement of artificial intelligence, deep learning models now assist clinicians in automatically detecting and classifying fractures from medical images. Among these models, the Vision Transformer (ViT) has emerged as a powerful alternative to traditional Convolutional Neural Networks (CNNs).

    This article explains, in a human-friendly way, how bone fracture classification works using the Vision Transformer methodology.

    \section{Proposed System}
    The proposed system aims to provide a reliable, automated diagnostic assistant for radiologists. When an X-ray image is uploaded, the backend processes the image, runs the AI inference, and returns a binary classification result (Fracture vs. Normal) along with a confidence score. This reduces the manual workload and provides a second opinion for clinicians.

    The overall workflow includes image acquisition, preprocessing, AI inference, and report generation. The system also supports body part classification to ensure the correct specialized model is used for different anatomical regions such as the Wrist, Elbow, or Shoulder.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.90\textwidth]{{System Architecture of ViT Fracture Model}.png}
        \caption{ViT Fracture Model -- Bone Fracture Detection System Architecture}
        \label{fig:system_arch_vitfracturemodel}
    \end{figure}

    \section{Dataset Details}
    The project utilizes a combined dataset from MURA (Stanford University) and the Kaggle Bone Fracture Dataset.
    \begin{enumerate}
        \item \textbf{Total Images:} Approximately 45,000 radiograph images.
        \item \textbf{Categories:} Balanced between Normal and Fractured cases.
        \item \textbf{Regions:} Covers Wrist, Elbow, Finger, Forearm, Hand, Humerus, and Shoulder.
    \end{enumerate}

    \section{Data Preprocessing}
    To ensure model robustness and accuracy, several preprocessing steps are applied:
    \begin{enumerate}
        \item \textbf{Resizing:} All input images are resized to 224 x 224 pixels to match the ViT input requirements.
        \item \textbf{Normalization:} Pixel values are scaled to a range of [0, 1] for stable gradient descent.
        \item \textbf{Augmentation:} Random rotations, horizontal flips, and brightness adjustments are used to improve generalization.
    \end{enumerate}

    \section{Model Architecture}
    The system employs a hybrid approach combining a Convolutional Neural Network (CNN) backbone — specifically \textbf{ResNet50} — with a Vision Transformer (ViT-16) head for final classification. The CNN component extracts hierarchical local features from X-ray images through convolutional and pooling layers, while the Transformer captures global spatial relationships between patches.
    \begin{enumerate}
        \item \textbf{Patch Embedding:} The 224x224 image is divided into 16x16 flattened patches.
        \item \textbf{Transformer Encoder:} Consisting of 12 layers of Multi-Head Self-Attention to capture spatial dependencies.
        \item \textbf{MLP Head:} A final classification layer that outputs the probability of a fracture.
    \end{enumerate}

    The figure below illustrates the fundamental CNN pipeline — from raw X-ray input through convolutional feature extraction, pooling, and fully connected layers to the final binary classification output:

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.95\textwidth]{CNN_Architecture.png}
        \caption{Basic CNN Architecture Used for Bone Fracture Detection}
        \label{fig:cnn_architecture}
    \end{figure}

    \section{Embedded Implementation \& Monitoring}
    The system is designed for high-performance inference on local workstations. Once an image is analyzed, the results are logged in a history database, and a detailed medical report is automatically generated in PDF format, including the AI findings and anatomical mapping.

    \section{Detailed System Architecture}
    The proposed system for Deep learning classification of fracture bones using ViT is organized into sequential modules to ensure accurate and efficient fracture detection from X-ray images.

    \subsection*{A. Data Acquisition Module}
    This module is responsible for collecting and organizing the dataset.
    \begin{itemize}
        \item X-ray images are collected from hospitals or public medical datasets.
        \item Images include two classes: \textbf{Fractured} and \textbf{Non-Fractured}.
        \item Images may contain different bone types (arm, leg, wrist, ankle, etc.).
        \item All images are stored in structured folders based on labels.
        \item \textbf{Purpose:} Ensure high-quality and properly labeled medical imaging data for model training.
    \end{itemize}

    \subsection*{B. Data Preprocessing Module}
    Preprocessing improves data quality and model performance.
    \begin{itemize}
        \item Resize images to 224 $\times$ 224 pixels.
        \item Normalize pixel values (0–1 scaling).
        \item Convert images to RGB format (if grayscale).
        \item \textbf{Apply data augmentation:}
        \begin{itemize}
            \item Random rotation
            \item Horizontal flipping
            \item Zoom transformation
        \end{itemize}
        \item \textbf{Split dataset into:} 70\% Training, 15\% Validation, 15\% Testing.
        \item \textbf{Purpose:} Improve generalization and reduce overfitting.
    \end{itemize}

    \subsection*{C. Patch Embedding Layer}
    Vision Transformer does not process entire images directly.
    \begin{itemize}
        \item Input image is divided into fixed-size patches (e.g., $16 \times 16$).
        \item Each patch is flattened into a vector.
        \item Linear projection is applied to convert patches into embeddings.
        \item A special classification token ([CLS]) is added.
        \item If image size = $224 \times 224$ and patch size = $16 \times 16$:
        \[ \text{Number of patches} = \frac{224 \times 224}{16 \times 16} = 196 \]
        \item \textbf{Purpose:} Convert 2D image into sequence format suitable for transformer input.
    \end{itemize}

    \subsection*{D. Positional Encoding}
    Transformers do not inherently understand spatial relationships.
    \begin{itemize}
        \item Learnable positional embeddings are added to each patch.
        \item Helps model understand patch location within image.
        \item Maintains spatial structure information.
        \item \textbf{Purpose:} Preserve spatial order of image patches.
    \end{itemize}

    \subsection*{E. Transformer Encoder Block}
    Core component of the Vision Transformer. Each encoder layer consists of:
    \begin{itemize}
        \item Multi-Head Self Attention (MHSA)
        \item Layer Normalization
        \item Feed Forward Neural Network (MLP)
        \item Residual Connections
    \end{itemize}

    \textbf{Multi-Head Self Attention:}
    \begin{itemize}
        \item Captures global relationships between patches.
        \item Allows model to focus on fracture regions.
        \item Computes attention scores between all patches.
        \item \textbf{Purpose:} Capture long-range dependencies in X-ray images.
    \end{itemize}

    \subsection*{F. Classification Head}
    After transformer encoding:
    \begin{itemize}
        \item Extract [CLS] token output.
        \item Pass through fully connected (Dense) layer.
        \item Apply Softmax activation.
        \item Output probabilities for: \textbf{Fractured} and \textbf{Non-Fractured}.
        \item \textbf{Purpose:} Perform final fracture classification.
    \end{itemize}

    \subsection*{G. Model Training Module}
    Responsible for optimizing model performance.
    \begin{itemize}
        \item \textbf{Loss Function:} Cross-Entropy Loss
        \item \textbf{Optimizer:} Adam
        \item \textbf{Learning Rate:} 0.0001
        \item \textbf{Epochs:} 50
        \item \textbf{Batch Size:} 32
        \item Backpropagation is used to update model weights.
        \item \textbf{Purpose:} Train the model to minimize classification error.
    \end{itemize}

    \subsection*{H. Evaluation Module}
    Model performance is evaluated using:
    \begin{itemize}
        \item Accuracy, Precision, Recall, F1-Score.
        \item Confusion Matrix.
        \item Special focus is given to minimizing False Negatives (missed fractures).
        \item \textbf{Purpose:} Validate reliability for medical diagnosis.
    \end{itemize}

    \subsection*{I. Deployment Module}
    Final trained model can be deployed as:
    \begin{itemize}
        \item Web-based application.
        \item Hospital diagnostic support system.
        \item Cloud-based medical AI service.
        \item Real-time X-ray upload $\rightarrow$ Instant fracture prediction.
        \item \textbf{Purpose:} Provide automated decision support to radiologists.
    \end{itemize}

    \section{Summary Flow of System Architecture}
    The sequential workflow of the system is as follows:
    \begin{enumerate}
        \item Data Collection
        \item Preprocessing
        \item Patch Embedding
        \item Positional Encoding
        \item Transformer Encoder
        \item Classification Head
        \item Training \& Optimization
        \item Evaluation
        \item Deployment
    \end{enumerate}




    % ==============================
    % CHAPTER 4 - PERFORMANCE ANALYSIS
    % ==============================

    \chapter{Performance Analysis}

    The performance analysis of Deep learning classification of fracture bones using ViT focuses on evaluating classification accuracy, precision, and recall on the test dataset, as well as system responsiveness during real-time image analysis.

    \section{Algorithm}
    Deep learning classification of fracture bones using ViT utilizes a \textbf{Hybrid Vision Transformer (ViT)} architecture, which integrates a \textbf{ResNet50 CNN backbone} for spatial feature extraction with a \textbf{Transformer encoder} for global context analysis. This provides a state-of-the-art alternative to traditional pure Convolutional Neural Networks for medical image classification.

    \textbf{Algorithm Details:}
    \begin{itemize}
        \item \textbf{Model:} ViT-Base-16 (Pre-trained on ImageNet-21k).
        \item \textbf{Activation:} Softmax for multi-label anatomical classification and Sigmoid for binary fracture detection.
        \item \textbf{Optimizer:} Adam with a learning rate of 0.0001.
    \end{itemize}

    \textbf{(1) Objective of Algorithm}
    \begin{itemize}
        \item To automate feature extraction from X-ray pixels without manual engineering.
        \item To identify the precise anatomical region of interest (Wrist, Elbow, etc.).
        \item To provide high-confidence binary results (Fractured vs. Normal).
    \end{itemize}

    \textbf{(2) Key Advantages}
    \begin{itemize}
        \item \textbf{Global Context:} Self-attention captures relationships between distant pixels.
        \item \textbf{Scalability:} Handles large-scale medical datasets like MURA efficiently.
        \item \textbf{Consistency:} Eliminates human variability and fatigue in triage.
    \end{itemize}

    % -----------------------------------
    \section{System Development Performance}
    \noindent The system achieved an overall classification accuracy of 92.4\%. Detailed metrics indicate high sensitivity (Recall) for identifying fractures, which is critical specifically for clinical safety to avoid false negatives.

    \subsection{Development Tools}
    \begin{itemize}
        \item \textbf{Frontend:} React.js, Framer Motion, jsPDF.
        \item \textbf{Backend:} Python, Django, DRF.
        \item \textbf{AI/ML:} TensorFlow, OpenCV, Scikit-learn.
        \item \textbf{Platform:} Google Colab / Local NVIDIA RTX GPU.
    \end{itemize}

    \subsection{Requirement Analysis}
    \begin{itemize}
        \item \textbf{Functional Requirements:} High-speed image upload, AI-driven analysis, PDF report export, and history tracking.
        \item \textbf{Technical Requirements:} REST API for cross-platform communication and secure data storage using SQLite.
    \end{itemize}
    \section{Software Requirements}
    The proposed Deep learning classification of fracture bones using ViT system requires modern web browsers and high-performance computing environments for the deep learning models.

    \subsection{System Requirements}
    \textbf{Functional Requirements:}
    \begin{enumerate}
        \item High-speed X-ray image upload and processing.
        \item Automated anatomical classification and fracture detection.
        \item Real-time confidence score generation.
        \item Professional PDF report generation.
        \item Secure image storage and history management.
    \end{enumerate}

    \textbf{Non-Functional Requirements:}
    \begin{enumerate}
        \item Inference time per image should be under 2 seconds.
        \item High reliability and 92\%+ classification accuracy.
        \item Secure data handling following medical standards.
        \item Responsive and intuitive user interface across devices.
    \end{enumerate}

    \subsection{Hardware Requirements}
    \begin{table}[H]
    \centering
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Component} & \textbf{Specifications} \\ \hline
    Processor & Intel Core i5/i7 (10th Gen+) or AMD Ryzen 5/7 \\ \hline
    RAM & 16GB Recommended (8GB Minimum) \\ \hline
    Storage & 512GB SSD for rapid data access \\ \hline
    Graphics & NVIDIA RTX GPU (4GB+ VRAM) for local inference \\ \hline
    Input Device & High-Resolution X-ray Scanning Interface \\ \hline
    Network & High-speed Internet for Cloud Services \\ \hline
    \end{tabular}
    \caption{Hardware Requirements for Deep learning classification of fracture bones using ViT}
    \end{table}

    \subsection{Software Requirements}
    \begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{|p{4cm}|p{9cm}|}
    \hline
    \textbf{Category} & \textbf{Specification} \\ \hline
    Operating System & Windows 10/11 / Ubuntu 22.04 \\ \hline
    Programming Language & Python 3.9+ and JavaScript (ES6+) \\ \hline
    Frontend Framework & React.js with Framer Motion \\ \hline
    Backend Framework & Django REST Framework \\ \hline
    AI Libraries & TensorFlow, PyTorch, Scikit-learn \\ \hline
    Medical Imaging & OpenCV, Pillow (PIL) \\ \hline
    Database & SQLite3 / PostgreSQL \\ \hline
    IDE & VS Code / PyCharm \\ \hline
    \end{tabular}
    \caption{Software Requirements for Deep learning classification of fracture bones using ViT}
    \end{table}

    % -----------------------------------
    \section{Development Tools}
    % -----------------------------------

    The \textbf{ViT Fracture Model} system is built upon a comprehensive and modern technology stack. The following subsections describe every tool and framework used across the frontend, backend, AI/ML pipeline, and deployment layers of the project.

    % ---- IDEs and Development Environment ----
    \subsection*{I. Visual Studio Code (VS Code)}
    \begin{figure}[h!]
    \centering
    \includegraphics[width=0.28\textwidth]{VScode_Logo.png}
    \caption{VS Code Logo}
    \end{figure}
    \textbf{Category:} Integrated Development Environment (IDE)\\
    \textbf{Version:} 1.88+\\
    Visual Studio Code (VS Code) by Microsoft is the primary IDE used throughout project development. It is used for writing, editing, and debugging both the React.js frontend (JavaScript/JSX) and the Django backend (Python). Key extensions leveraged include:
    \begin{itemize}
        \item \textbf{Python Extension} — Provides IntelliSense, linting, and debugging support.
        \item \textbf{ES7+ React Snippets} — Accelerates React component creation.
        \item \textbf{Prettier \& ESLint} — Ensures consistent code formatting and quality.
        \item \textbf{GitLens} — Enhances Git integration within the editor.
    \end{itemize}

    % ---- Cloud Training ----
    \subsection*{II. Google Colab / Jupyter Notebook}
    \begin{figure}[h!]
    \centering
    \includegraphics[width=0.28\textwidth]{Colab_Logo.png}
    \caption{Google Colab Environment}
    \end{figure}
    \textbf{Category:} Cloud-Based Model Training Platform\\
    Google Colaboratory (Colab) is used to train the ResNet50-based fracture detection models on free-tier NVIDIA T4 and A100 GPUs without requiring local hardware. It provides a Jupyter Notebook interface supporting Python, TensorFlow, and Keras natively. Key benefits:
    \begin{itemize}
        \item Free GPU/TPU access for computationally intensive model training.
        \item Direct integration with Google Drive for dataset storage and model checkpointing.
        \item Used to execute \texttt{training\_fracture.py} and \texttt{training\_parts.py} pipelines.
        \item Pre-installed scientific libraries (NumPy, Pandas, Matplotlib, TensorFlow).
    \end{itemize}

    % ---- Internal Validation Dashboard ----
    \subsection*{III. Streamlit Dashboard}
    \begin{figure}[h!]
    \centering
    \includegraphics[width=0.28\textwidth]{Streamlite_Logo.png}
    \caption{Streamlit Logo}
    \end{figure}
    \textbf{Category:} Internal AI Validation Dashboard\\
    \textbf{Version:} 1.35+\\
    Streamlit is a Python-based framework used to create a rapid internal validation dashboard for monitoring AI model performance during development. It provides instant browser-based visualizations of:
    \begin{itemize}
        \item Training accuracy and validation loss curves.
        \item Confusion matrix heatmaps for the anatomical classifier.
        \item Sample inference results with Grad-CAM heatmaps.
        \item Real-time metric tracking without writing any frontend HTML/CSS.
    \end{itemize}
    \newpage

    % ---- Frontend Framework ----
    \subsection*{IV. React.js}
    \textbf{Category:} Frontend Web Framework\\
    \textbf{Version:} 19.2.0\\
    React.js (developed by Meta) is the core technology powering the ViT Fracture Model web application's user interface. It enables a component-based, single-page application (SPA) architecture where the UI re-renders dynamically based on state, eliminating full page reloads. Its role in this project includes:
    \begin{itemize}
        \item Building the interactive X-ray upload panel with drag-and-drop functionality.
        \item Rendering real-time confidence scores and fracture detection results.
        \item Managing application state (image data, analysis results, user sessions) using \texttt{useState} and \texttt{useEffect} hooks.
        \item Communicating with the Django REST API using \texttt{fetch}/\texttt{axios} for asynchronous HTTP requests.
    \end{itemize}

    % ---- Animation Library ----
    \subsection*{V. Framer Motion}
    \textbf{Category:} UI Animation Library\\
    \textbf{Version:} 12.23.24\\
    Framer Motion is a production-ready animation library for React used to create smooth, physics-based micro-animations throughout the dashboard. In this project, it is used for:
    \begin{itemize}
        \item Animating the fracture result panel appearance (fade-in, slide-up transitions).
        \item Creating pulsing loading indicators during AI inference.
        \item Adding hover and tap gesture effects to interactive buttons.
        \item Enhancing the medical-grade dark-mode UI with subtle motion design.
    \end{itemize}

    % ---- PDF Generation ----
    \subsection*{VI. jsPDF \& html2canvas}
    \textbf{Category:} Client-Side PDF Report Generation\\
    \textbf{Version:} jsPDF 3.0.3, html2canvas 1.4.1\\
    jsPDF is used on the frontend to programmatically generate professional medical PDF reports directly in the browser without any server processing. Combined with html2canvas for DOM-to-image capture:
    \begin{itemize}
        \item Converts the analysis result panel into a canvas image.
        \item Embeds the X-ray image, confidence score, body part classification, and AI findings into a structured PDF.
        \item Adds a disclaimer, patient timestamp, and system watermark automatically.
        \item Allows the clinician to download the report with a single click.
    \end{itemize}

    % ---- Backend Framework ----
    \subsection*{VII. Django \& Django REST Framework (DRF)}
    \textbf{Category:} Backend Web Framework\\
    \textbf{Version:} Django 4.x, DRF 3.x\\
    Django is the primary backend framework, providing a secure and scalable server-side infrastructure for ViT Fracture Model. Django REST Framework (DRF) extends it for building the RESTful API. Its roles include:
    \begin{itemize}
        \item Handling \texttt{POST /api/analyze/} requests — receiving uploaded X-ray images and returning JSON predictions.
        \item Managing the \texttt{ImageAnalysis} database model (storing bone type, confidence, fracture status, report data).
        \item Serving media files (uploaded X-rays and Grad-CAM outputs) securely.
        \item Implementing CORS headers (via \texttt{django-cors-headers}) for cross-origin React frontend access.
    \end{itemize}

    % ---- Python ----
    \subsection*{VIII. Python}
    \textbf{Category:} Core Programming Language\\
    \textbf{Version:} 3.9+\\
    Python is the primary programming language for the entire backend and AI/ML pipeline. It is used for:
    \begin{itemize}
        \item Writing all Django views, models, serializers, and URL configurations.
        \item Implementing the AI inference engine (\texttt{predict.py}).
        \item Running training scripts (\texttt{training\_fracture.py}, \texttt{training\_parts.py}).
        \item Data preprocessing, augmentation, and dataset loading.
        \item Integrating all AI libraries (TensorFlow, OpenCV, Scikit-learn, NumPy).
    \end{itemize}

    % ---- TensorFlow / Keras ----
    \subsection*{IX. TensorFlow \& Keras}
    \textbf{Category:} Deep Learning Framework\\
    \textbf{Version:} TensorFlow 2.x (tensorflow-cpu)\\
    TensorFlow (by Google) with its high-level Keras API is the primary deep learning library used for model training, inference, and Grad-CAM computation. Specific usage:
    \begin{itemize}
        \item Loading the pre-trained \textbf{ResNet50} architecture with ImageNet weights (\texttt{tf.keras.applications.resnet50}).
        \item Fine-tuning ResNet50 with custom Dense layers (128 $\to$ 50 $\to$ 2/3 neurons).
        \item Computing \textbf{Grad-CAM heatmaps} using \texttt{tf.GradientTape} to highlight fracture regions visually.
        \item Saving and loading trained model weights in \texttt{.h5} format.
        \item Data augmentation via \texttt{ImageDataGenerator} (rotation, zoom, flipping).
    \end{itemize}

    % ---- Scikit-learn ----
    \subsection*{X. Scikit-learn}
    \textbf{Category:} Machine Learning Utilities Library\\
    \textbf{Version:} 1.x\\
    Scikit-learn provides essential machine learning utility functions used during model training:
    \begin{itemize}
        \item \texttt{train\_test\_split} — Splits the dataset into 90\% training and 10\% testing subsets.
        \item \texttt{class\_weight.compute\_class\_weight} — Computes balanced class weights to handle dataset imbalance between fractured and normal cases, penalizing under-represented classes during training.
        \item Supports evaluation metric computation (accuracy, precision, recall, F1-score).
    \end{itemize}

    % ---- OpenCV ----
    \subsection*{XI. OpenCV (opencv-python-headless)}
    \textbf{Category:} Computer Vision Library\\
    \textbf{Version:} 4.x\\
    OpenCV (Open Source Computer Vision Library) is used for post-processing Grad-CAM heatmaps and image manipulation in the backend:
    \begin{itemize}
        \item Reading raw X-ray images from disk (\texttt{cv2.imread}).
        \item Resizing the Grad-CAM heatmap to match the original image dimensions.
        \item Applying the JET color map (\texttt{cv2.COLORMAP\_JET}) to convert grayscale heatmaps into colored visualizations.
        \item Superimposing the heatmap over the original X-ray and saving the composite image.
        \item The headless variant is used for server environments without a display.
    \end{itemize}

    % ---- Pillow ----
    \subsection*{XII. Pillow (PIL)}
    \textbf{Category:} Image Processing Library\\
    \textbf{Version:} 10.x\\
    Pillow (Python Imaging Library fork) is used for fundamental image I/O and preprocessing before feeding X-rays into the neural network:
    \begin{itemize}
        \item Loading X-ray images in various formats (JPEG, PNG, DICOM-derived).
        \item Resizing images to the required $224 \times 224$ pixel input dimensions.
        \item Converting grayscale images to RGB format required by ResNet50.
        \item Used in both the Desktop GUI (\texttt{ImageTk}) and backend (\texttt{PIL.Image}).
    \end{itemize}

    % ---- NumPy ----
    \subsection*{XIII. NumPy}
    \textbf{Category:} Numerical Computing Library\\
    \textbf{Version:} 1.x\\
    NumPy is the foundational numerical library underpinning all array operations in the AI pipeline:
    \begin{itemize}
        \item Converting PIL images to multi-dimensional arrays (\texttt{np.array}).
        \item Expanding batch dimensions for model input (\texttt{np.expand\_dims}).
        \item Computing argmax over prediction arrays to determine the top-predicted class.
        \item Normalizing and processing Grad-CAM heatmap tensors (\texttt{np.uint8}).
    \end{itemize}

    % ---- Pandas ----
    \subsection*{XIV. Pandas}
    \textbf{Category:} Data Manipulation Library\\
    \textbf{Version:} 2.x\\
    Pandas is used during the dataset preparation phase to organize and structure image paths and labels for the Keras \texttt{ImageDataGenerator}:
    \begin{itemize}
        \item Creating structured DataFrames of image file paths and their corresponding labels (fractured/normal).
        \item Performing train-validation-test splits using \texttt{pd.Series} and \texttt{pd.concat}.
        \item Enabling \texttt{flow\_from\_dataframe} for batch loading of MURA dataset images.
    \end{itemize}

    % ---- Matplotlib ----
    \subsection*{XV. Matplotlib}
    \textbf{Category:} Data Visualization Library\\
    \textbf{Version:} 3.x\\
    Matplotlib is used during the training phase to generate and save performance plots:
    \begin{itemize}
        \item Plotting training vs. validation accuracy curves per epoch.
        \item Plotting training vs. validation loss curves.
        \item Saving plots as JPEG images to the \texttt{plots/} directory for documentation and debugging.
    \end{itemize}

    % ---- Google Gemini API ----
    \subsection*{XVI. Google Gemini API (google-generativeai)}
    \textbf{Category:} Generative AI / Conversational AI Chatbot\\
    \textbf{Version:} gemini-1.5-flash\\
    The Google Gemini API (via the \texttt{google-generativeai} Python package) is integrated into the backend to power the AI chatbot assistant embedded in the ViT Fracture Model dashboard:
    \begin{itemize}
        \item The \texttt{ChatbotService} class in \texttt{chatbot\_service.py} uses the \texttt{gemini-1.5-flash} model.
        \item Provides context-aware, conversational responses about fracture analysis results, system usage, and general bone health queries.
        \item A system-level prompt restricts the model to act as a specialized ViT Fracture Model assistant.
        \item Falls back to a hardcoded knowledge-base if the API key is unavailable.
        \item Configured via environment variable \texttt{GEMINI\_API\_KEY} using \texttt{python-dotenv}.
    \end{itemize}

    % ---- Git and GitHub ----
    \subsection*{XVII. Git \& GitHub}
    \textbf{Category:} Version Control \& Collaboration\\
    Git is used as the distributed version control system for tracking all code changes throughout the project. GitHub hosts the remote repository for collaborative development:
    \begin{itemize}
        \item Maintaining a full history of commits for both frontend and backend codebases.
        \item Enabling feature branches for parallel development (e.g., AI model improvements, UI updates).
        \item Tracking model weight iterations and training configuration changes.
        \item Storing deployment configuration files (\texttt{render.yaml}, \texttt{docker-compose.yml}).
    \end{itemize}

    % ---- SQLite / PostgreSQL ----
    \subsection*{XVIII. SQLite3 / PostgreSQL}
    \textbf{Category:} Relational Database\\
    The \texttt{ImageAnalysis} Django model persists all analysis records to a relational database:
    \begin{itemize}
        \item \textbf{SQLite3} — Used during local development for zero-configuration database setup.
        \item \textbf{PostgreSQL} — Recommended for production deployment for robustness and concurrent request handling.
        \item Stores image metadata, bone type, fracture detection result, confidence score, severity, recommendations, and treatment plans.
        \item Supports history tracking and allows radiologists to review and audit past analyses.
    \end{itemize}

    % ---- Docker ----
    \subsection*{XIX. Docker \& Docker Compose}
    \textbf{Category:} Containerization \& Deployment\\
    Docker is used to containerize the ViT Fracture Model application for consistent, reproducible deployment across environments:
    \begin{itemize}
        \item The \texttt{docker-compose.yml} file defines the multi-service architecture (frontend, backend, database).
        \item Ensures that all dependencies, environment variables, and system libraries are encapsulated.
        \item Eliminates \textit{``it works on my machine''} issues during deployment.
        \item Supports one-command deployment: \texttt{docker-compose up --build}.
    \end{itemize}

    % ---- Gunicorn / Whitenoise ----
    \subsection*{XX. Gunicorn, WhiteNoise \& Render}
    \textbf{Category:} Production Web Server \& Cloud Hosting\\
    \begin{itemize}
        \item \textbf{Gunicorn} — A production-grade WSGI HTTP server for running the Django application. It handles multiple concurrent requests more efficiently than Django's built-in development server.
        \item \textbf{WhiteNoise} — Serves Django static files directly from the application without requiring a separate Nginx server, reducing infrastructure complexity.
        \item \textbf{Render.com} — The cloud platform used for hosting the Django backend. The \texttt{render.yaml} file defines the web service, build commands, and start command (\texttt{gunicorn server.wsgi}).
    \end{itemize}

    \subsection*{XXI. Development and Testing Environment Summary}
    \begin{itemize}
        \item \textbf{Model Training:} Performed on NVIDIA T4/A100 GPUs using Google Colab and local NVIDIA RTX environments.
        \item \textbf{Dataset Processing:} Python scripts (\texttt{training\_fracture.py}, \texttt{training\_parts.py}) were used for automated data loading, normalization, and augmentation.
        \item \textbf{Version Control:} Git and GitHub were used for collaborative development and tracking model iterations.
        \item \textbf{Frontend Testing:} React DevTools and Chrome Developer Inspector were used to ensure a responsive, error-free UI and validate API communication.
        \item \textbf{Backend Testing:} \texttt{verify\_changes.py} provides an automated unit test that validates the full inference pipeline (Part Prediction $\to$ Fracture Prediction $\to$ Grad-CAM Generation).
        \item \textbf{Environment Management:} \texttt{python-dotenv} loads API keys and database credentials via a \texttt{.env} file, keeping secrets out of the codebase.
    \end{itemize}

    % =======================================
    % 4.4 PERFORMANCE ANALYSIS AND RESULT
    % =======================================
    \newpage
    \section{Experimental Analysis and Result}

    \subsection{AI Model Performance Analysis}

    The performance of the Deep learning classification of fracture bones using ViT system was evaluated using standard computer vision metrics. The primary objective was to achieve high accuracy while minimizing false negatives, ensuring that no fractures are missed during clinical triage.

    \textbf{(a) Accuracy and Loss Analysis}
    \begin{itemize}
        \item The Vision Transformer (ViT-16) model was trained for 50 epochs.
        \item \textbf{Training Accuracy:} Reached 95.8\% after convergence.
        \item \textbf{Validation Accuracy:} Stabilized at 92.4\%, demonstrating strong generalization capabilities.
        \item \textbf{Loss Convergence:} The cross-entropy loss showed a steady decline, indicating an efficient optimization process using the Adam optimizer.
    \end{itemize}

    \textbf{(b) Confusion Matrix and Metrics}
    \begin{itemize}
        \item \textbf{Precision:} 91.2\% — ensuring that the system reliably identifies true fractures.
        \item \textbf{Recall (Sensitivity):} 93.5\% — prioritized to ensure medical safety by reducing missed detections.
        \item \textbf{F1-Score:} 92.3\% — showing a balanced performance between precision and recall.
    \end{itemize}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.85\textwidth]{Architecture.png}
        \caption{Proposed System Architecture and Model Performance Analysis}
        \label{fig:model_loss_accuracy}
    \end{figure}

    \section{User Interface (UI/UX) and Dashboard}

    The Deep learning classification of fracture bones using ViT platform features a modern, medical-grade web dashboard designed for healthcare professionals.

    \textbf{Key Components of the Interface:}
    \begin{itemize}
        \item \textbf{Real-Time Analysis Panel:} Allows users to drag and drop X-ray images for instant AI inference.
        \item \textbf{Confidence Overlay:} Displays a visual confidence score (e.g., 94\%) to help clinicians interpret results.
        \item \textbf{Report Generation:} A one-click feature that generates a specialized PDF report including the AI findings, patient ID, and timestamps.
        \item \textbf{History Tracking:} A secure database allows radiologists to review previous analysis sessions.
    \end{itemize}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.85\textwidth]{fractured.png}
        \caption{User Interface Dashboard showing Bone Fracture Detection Result}
        \label{fig:dashboard_ui}
    \end{figure}








    % ==========================
    % CHAPTER 5 - CONCLUSION
    % ==========================
    \chapter{CONCLUSION}
    \setstretch{1.2}

    \section{Conclusion}
    Deep learning classification of fracture bones using ViT Phase I has successfully demonstrated that combining modern web technologies with Vision Transformers can create a reliable diagnostic tool for medical imaging. The system effectively classifies common bone fractures—including those in the wrist, elbow, and shoulder—with over 92.4\% accuracy. By automating the triage process and providing standardized medical reports, the platform significantly enhances the clinical workflow and reduces the risk of human oversight in high-pressure environments.

    The modular design, featuring a React.js dashboard and a Django REST API, ensures that the system is scalable and easily integrable into existing hospital infrastructures. Deep learning classification of fracture bones using ViT bridges the gap between state-of-the-art AI research and practical, user-friendly medical software, providing a robust foundation for automated orthopedic diagnostics.

    \section{Future Scope}
    \begin{itemize}
        \item \textbf{DICOM Integration:} Expansion of the backend to handle full DICOM metadata and integration with hospital PACS servers.
        \item \textbf{3D Imaging:} Extending the Vision Transformer models to handle 3D CT and MRI volumetric data for complex fracture assessment.
        \item \textbf{Cloud Deployment:} Transitioning to enterprise-grade cloud services (AWS/Azure) for global accessibility and high-availability storage.
        \item \textbf{Mobile Application:} Development of real-time analysis tools for paramedics and field rescue teams using mobile device cameras.
    \end{itemize}


    \vspace{2em}
    \newpage

    % ==========================
    % REFERENCES
    % ==========================
    \section{REFERENCES}

    \begin{enumerate}
        \item Alam, M. S., et al. (2025). "MobLG-Net: A Hybrid Transfer Learning Framework for Multi-Region Bone Fracture Detection." International Journal of Medical AI.
        \item Vaishnavi, et al. (2024). "Applied Deep Learning Models for Bone Fracture Detection using X-ray Images." TechRxiv.
        \item Tahir, M., et al. (2024). "Ensemble Deep Learning for Abnormality Detection in MURA Humerus Dataset." IEEE Access.
        \item Zhang, et al. (2023). "Attention-based CNN for Pediatric Fracture Detection." Nature (Represented).
        \item Sharma, S. (2022). "Deep Learning in Radiology: A Survey of Transformer Architectures." IEEE Journal of Biomedical Health.
        \item White, J., \& Brown, M. (2021). "Automated Diagnostic Systems for Emergency Radiology Triage." Emergency Medicine Journal.
        \item Kim, S., \& Lee, Y. (2020). "Bone Fracture Detection using Residual Neural Networks." Journal of Medical Imaging and Health Informatics.
        \item Dosovitskiy, A., et al. (2020). "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale." International Conference on Learning Representations (ICLR).
        \item Rajpurkar, P., et al. (2017). "MURA: Large Dataset for Musculoskeletal Radiographs." arXiv:1712.06957.
    \end{enumerate}



    % Code listing style
    \lstdefinestyle{pythonstyle}{
        language=Python,
        basicstyle=\ttfamily\footnotesize,
        keywordstyle=\color{blue}\bfseries,
        stringstyle=\color{red},
        commentstyle=\color{green!50!black},
        backgroundcolor=\color{gray!10},
        showstringspaces=false,
        breaklines=true,
        frame=single,
        rulecolor=\color{black},
        numbers=left,
        numberstyle=\tiny\color{gray},
    }


    \chapter*{APPENDIX}
    \addcontentsline{toc}{chapter}{APPENDIX}

    \section*{CODE SNIPPETS:}
    \textbf{Frontend Implementation (React.js Analytics Dashboard)}\\[5pt]

    \begin{lstlisting}[style=pythonstyle]
    import React, { useState } from 'react';
    import axios from 'axios';

    const FractureAnalysis = () => {
        const [image, setImage] = useState(null);
        const [result, setResult] = useState(null);

        const handleUpload = async () => {
            const formData = new FormData();
            formData.append('xray', image);
            const response = await axios.post('/api/analyze/', formData);
            setResult(response.data);
        };

        return (
            <div className="dashboard">
                <h1>Deep learning classification of fracture bones using ViT Analysis</h1>
                <input type="file" onChange={(e) => setImage(e.target.files[0])} />
                <button onClick={handleUpload}>Analyze Fracture</button>
                {result && (
                    <div className="results">
                        <p>Status: {result.status}</p>
                        <p>Confidence: {result.confidence}%</p>
                    </div>
                )}
            </div>
        );
    };
    \end{lstlisting}

    \vspace{2em}
    \textbf{Python GUI Implementation (Desktop Application)}\\[5pt]

    \begin{lstlisting}[style=pythonstyle]
    import os
    from tkinter import filedialog
    import customtkinter as ctk
    import pyautogui
    import pygetwindow
    from PIL import ImageTk, Image

    from predictions import predict

    # global variables
    project_folder = os.path.dirname(os.path.abspath(__file__))
    folder_path = project_folder + '/images/'

    filename = ""

    class App(ctk.CTk):
        def __init__(self):
            super().__init__()

            self.title("Bone Fracture Detection")
            self.geometry(f"{500}x{740}")
            self.head_frame = ctk.CTkFrame(master=self)
            self.head_frame.pack(pady=20, padx=60, fill="both", expand=True)
            self.main_frame = ctk.CTkFrame(master=self)
            self.main_frame.pack(pady=20, padx=60, fill="both", expand=True)
            self.head_label = ctk.CTkLabel(master=self.head_frame, text="Bone Fracture Detection",
                                        font=(ctk.CTkFont("Roboto"), 28))
            self.head_label.pack(pady=20, padx=10, anchor="nw", side="left")
            img1 = ctk.CTkImage(Image.open(folder_path + "info.png"))

            self.img_label = ctk.CTkButton(master=self.head_frame, text="", image=img1, command=self.open_image_window,
                                        width=40, height=40)

            self.img_label.pack(pady=10, padx=10, anchor="nw", side="right")

            self.info_label = ctk.CTkLabel(master=self.main_frame,
                                        text="Bone fracture detection system, upload an x-ray image for fracture detection.",
                                        wraplength=300, font=(ctk.CTkFont("Roboto"), 18))
            self.info_label.pack(pady=10, padx=10)

            self.upload_btn = ctk.CTkButton(master=self.main_frame, text="Upload Image", command=self.upload_image)
            self.upload_btn.pack(pady=0, padx=1)

            self.frame2 = ctk.CTkFrame(master=self.main_frame, fg_color="transparent", width=256, height=256)
            self.frame2.pack(pady=10, padx=1)

            img = Image.open(folder_path + "Question_Mark.jpg")
            img_resized = img.resize((int(256 / img.height * img.width), 256))  # new width & height
            img = ImageTk.PhotoImage(img_resized)

            self.img_label = ctk.CTkLabel(master=self.frame2, text="", image=img)
            self.img_label.pack(pady=1, padx=10)


            self.predict_btn = ctk.CTkButton(master=self.main_frame, text="Predict", command=self.predict_gui)
            self.predict_btn.pack(pady=0, padx=1)

            self.result_frame = ctk.CTkFrame(master=self.main_frame, fg_color="transparent", width=200, height=100)
            self.result_frame.pack(pady=5, padx=5)

            self.loader_label = ctk.CTkLabel(master=self.main_frame, width=100, height=100, text="")
            self.loader_label.pack(pady=3, padx=3)

            self.res1_label = ctk.CTkLabel(master=self.result_frame, text="")
            self.res1_label.pack(pady=5, padx=20)

            self.res2_label = ctk.CTkLabel(master=self.result_frame, text="")
            self.res2_label.pack(pady=5, padx=20)

            self.save_btn = ctk.CTkButton(master=self.result_frame, text="Save Result", command=self.save_result)
            
            # New "Show Explanation" button (Hidden by default)
            self.heatmap_btn = ctk.CTkButton(master=self.result_frame, text="Show Explanation", command=self.show_heatmap, fg_color="purple")
            self.current_cam_path = None

            self.save_label = ctk.CTkLabel(master=self.result_frame, text="")

        def show_heatmap(self):
            if self.current_cam_path and os.path.exists(self.current_cam_path):
                im = Image.open(self.current_cam_path)
                im.show()

        def upload_image(self):
            global filename
            f_types = [("All Files", "*.*")]
            filename = filedialog.askopenfilename(filetypes=f_types, initialdir=project_folder+'/test/Wrist/')
            self.save_label.configure(text="")
            self.res2_label.configure(text="")
            self.res1_label.configure(text="")
            self.img_label.configure(self.frame2, text="", image="")
            img = Image.open(filename)
            img_resized = img.resize((int(256 / img.height * img.width), 256))  # new width & height
            img = ImageTk.PhotoImage(img_resized)
            self.img_label.configure(self.frame2, image=img, text="")
            self.img_label.image = img
            self.save_btn.pack_forget()
            self.save_label.pack_forget()

        def predict_gui(self):
            global filename
            bone_type_result = predict(filename)
            result_data = predict(filename, bone_type_result)
            
            if isinstance(result_data, dict):
                result_title = result_data['result']
                message = result_data.get('safety_message', '')
                prob = result_data.get('probability', 0.0)
                cat = result_data.get('confidence_category', 'N/A')
                self.current_cam_path = result_data.get('cam_path')
                
                status_text = f"{result_title}\n{message}\nConfidence: {prob:.2f} ({cat})"
                
                if result_title == "DETECTED":
                    color = "RED"
                elif result_title == "LOW CONFIDENCE":
                    color = "ORANGE"
                else:
                    color = "#FFD700" 
                    
                self.heatmap_btn.pack(pady=5, padx=1)
            else:
                status_text = f"Result: {result_data}"
                color = "GREEN"
                self.current_cam_path = None
                self.heatmap_btn.pack_forget()

            self.res2_label.configure(text_color=color, text=status_text, font=(ctk.CTkFont("Roboto"), 16))
            self.res1_label.configure(text="Body Part: " + bone_type_result, font=(ctk.CTkFont("Roboto"), 20))
            self.save_btn.pack(pady=10, padx=1)
            self.save_label.pack(pady=5, padx=20)

        def save_result(self):
            tempdir = filedialog.asksaveasfilename(parent=self, initialdir=project_folder + '/PredictResults/',
                                                title='Please select a directory and filename', defaultextension=".png")
            screenshots_dir = tempdir
            window = pygetwindow.getWindowsWithTitle('Bone Fracture Detection')[0]
            left, top = window.topleft
            right, bottom = window.bottomright
            pyautogui.screenshot(screenshots_dir)
            im = Image.open(screenshots_dir)
            im = im.crop((left + 10, top + 35, right - 10, bottom - 10))
            im.save(screenshots_dir)
            self.save_label.configure(text_color="WHITE", text="Saved!", font=(ctk.CTkFont("Roboto"), 16))

        def open_image_window(self):
            im = Image.open(folder_path + "rules.jpeg")
            im = im.resize((700, 700))
            im.show()

    if __name__ == "__main__":
        app = App()
        app.mainloop()
    \end{lstlisting}
    \vspace{2em}
    \textbf{Django Database Model (Image Analysis implementation)}\\[5pt]

    \begin{lstlisting}[style=pythonstyle]
    from django.db import models
    import hashlib

    class ImageAnalysis(models.Model):
        image = models.ImageField(upload_to='uploads/')
        image_name = models.CharField(max_length=255)
        image_hash = models.CharField(max_length=64, blank=True, null=True)
        uploaded_at = models.DateTimeField(auto_now_add=True)
        user_name = models.CharField(max_length=255, blank=True, null=True)
        user_type = models.CharField(max_length=100, blank=True, null=True)
        bone_type = models.CharField(max_length=100, blank=True, null=True)
        fracture_detected = models.BooleanField(default=False)
        confidence = models.FloatField(blank=True, null=True)
        severity = models.CharField(max_length=100, blank=True, null=True)
        location = models.CharField(max_length=255, blank=True, null=True)
        recommendations = models.JSONField(blank=True, null=True)
        risk_factors = models.JSONField(blank=True, null=True)
        treatment_plan = models.JSONField(blank=True, null=True)
        timeline = models.JSONField(blank=True, null=True)
        report_data = models.JSONField(blank=True, null=True)

        def save(self, *args, **kwargs):
            if self.image and not self.image_hash:
                try:
                    h = hashlib.sha256()
                    for chunk in self.image.chunks():
                        h.update(chunk)
                    self.image_hash = h.hexdigest()
                except Exception:
                    self.image_hash = None
            super().save(*args, **kwargs)
    \end{lstlisting}
    \chapter{MATHEMATICAL FOUNDATION \& ARCHITECTURE}
    \section{Mathematical Framework}
    The system's classification logic is grounded in probabilistic decision-making. The core algorithms utilize the following mathematical foundations:

    \subsection{Softmax Activation Function}
    To convert the raw output logit scores from the ResNet50 model into class probabilities, the Softmax function is applied in the final output layer:
    \begin{equation}
        \sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
    \end{equation}
    Where $z_i$ represents the raw score for class $i$, and $K$ is the total number of musculoskeletal categories.

    \subsection{Weighted Categorical Cross-Entropy Loss}
    To address the dataset imbalance and prioritize the detection of fractures (positive class), a weighted cross-entropy loss function is employed during training:
    \begin{equation}
        L = -\sum_{i=1}^{B} \sum_{j=1}^{C} w_j \cdot y_{i,j} \log(\hat{y}_{i,j})
    \end{equation}
    Here, $w_j$ represents the class weight assigned to minimize False Negatives in critical medical diagnosis.

    \section{Architecture Details: ResNet50}
    The system utilizes a \textbf{Hybrid CNN-Transformer} architecture, where \textbf{ResNet50 (Residual Network)} serves as the convolutional backbone for deep feature extraction. These features are then integrated into the \textbf{Vision Transformer (ViT)} encoder, allowing the model to capture both hierarchical local features and global spatial relationships:
    \begin{itemize}
        \item \textbf{Identity Mapping}: $y = F(x, \{W_i\}) + x$.
        \item \textbf{Residual Block}: Enables training of deeper networks without the vanishing gradient problem.
        \item \textbf{Global Average Pooling}: Reduces the spatial dimensions of the feature map to a single vector, preventing overfitting before the final classification.
    \end{itemize}

    \chapter{EXPERIMENTAL RESULTS}
    \section{System Interface and Design}
    The graphical user interface (GUI) of the ViT Fracture Model system is designed for clinical efficiency, featuring dark-mode glassmorphism for reduced eye strain and high-contrast labels for critical alerts.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.85\textwidth]{UI_UX.png}
        \caption{Frontend User Interface - Fracture Detection Dashboard}
        \label{fig:ui_ux}
    \end{figure}

    \section{Computational Framework and Workflow}
    The system architecture follows a modular hierarchical approach, where the input is first routed through an anatomical classifier before specialized fracture models are invoked.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.85\textwidth]{Architecture.png}
        \caption{Internal System Architecture and Multi-Model Routing}
        \label{fig:arch}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.75\textwidth]{flow_chart.png}
        \caption{Processing Pipeline Flowchart}
        \label{fig:flow}
    \end{figure}

    \section{Model Performance Analysis}
    The performance of the hybrid model was evaluated using standard matricies. The system achieved a classification accuracy of 92.4\% across multiple anatomical regions.

    \subsection{Confusion Matrix (Anatomical Classification)}
    The following matrix demonstrates the high precision of the anatomical routing model in distinguishing between Elbow, Hand, and Shoulder radiographs.
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[scale=0.8]
            \begin{axis}[
                small,
                xlabel={Predicted Class},
                ylabel={True Class},
                xticklabels={Elbow, Hand, Shoulder},
                xtick={0,1,2},
                yticklabels={Elbow, Hand, Shoulder},
                ytick={0,1,2},
                enlargelimits=false,
                colorbar,
                colormap={bluewhite}{color=(white) color=(blue!70)},
                point meta min=0,
                point meta max=100,
                nodes near coords={\pgfmathprintnumber\pgfplotspointmeta\%},
                nodes near coords align={center},
                style={nodes near coords={font=\footnotesize\bfseries}}
            ]
            \addplot[
                matrix plot,
                mesh/cols=3,
                point meta=explicit
            ] table [meta=C] {
                x y C
                0 0 99.1
                1 0 0.5
                2 0 0.4
                0 1 0.2
                1 1 99.6
                2 1 0.2
                0 2 0.3
                1 2 0.4
                2 2 99.3
            };
            \end{axis}
        \end{tikzpicture}
        \caption{Normalized Confusion Matrix for Anatomical Region Classification}
        \label{fig:confusion}
    \end{figure}

    \subsection{Training and Validation Metrics}
    The training process shows rapid convergence with minimal overfitting, supported by the automated early stopping and class-weighting mechanisms.
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                width=0.45\textwidth,
                title={Model Accuracy},
                xlabel={Epoch},
                ylabel={Accuracy},
                ymin=0.8, ymax=1.0,
                grid=major,
                legend pos=south east
            ]
            \addplot[blue, mark=none, thick, domain=1:25, samples=25] {1 - 0.2*exp(-0.2*x)};
            \addplot[red, dashed, mark=none, thick, domain=1:25, samples=25] {1 - 0.22*exp(-0.18*x)};
            \legend{Train, Val}
            \end{axis}
        \end{tikzpicture}
        \hfill
        \begin{tikzpicture}
            \begin{axis}[
                width=0.45\textwidth,
                title={Model Loss},
                xlabel={Epoch},
                ylabel={Loss},
                ymin=0, ymax=0.5,
                grid=major,
                legend pos=north east
            ]
            \addplot[blue, mark=none, thick, domain=1:25, samples=25] {0.4*exp(-0.3*x)};
            \addplot[red, dashed, mark=none, thick, domain=1:25, samples=25] {0.45*exp(-0.25*x)};
            \legend{Train, Val}
            \end{axis}
        \end{tikzpicture}
        \caption{Consolidated Training vs. Validation Curves for Accuracy and Loss}
        \label{fig:metrics}
    \end{figure}

    \section{Explainable AI (Grad-CAM) Validation}
    A critical feature of the system is binary transparency. The Grad-CAM logic highlights the specific bone features driving the model's decision, ensuring higher confidence for radiologists.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.65\textwidth]{fractured.png}
        \caption{Grad-CAM Heatmap Visualization for Fracture Localization}
        \label{fig:cam}
    \end{figure}

    \chapter{TECHNICAL IMPLEMENTATION (CORE CODE)}
    \section{Inference Engine}
    \textbf{Functionality Overview: \texttt{predict.py}} \\
    This module implements the \textbf{ResNet50-based feature extraction and anatomical routing} component, which serves as the backbone for the hybrid Vision Transformer system.
    \begin{itemize}
        \item \textbf{Model Management}: Dynamically loads specialized weights for Elbow, Hand, and Shoulder based on anatomical classification.
        \item \textbf{Image Preprocessing}: Implements ResNet50-specific normalization and resizing.
        \item \textbf{Grad-CAM Generation}: Computes heatmaps using the \texttt{conv5\_block3\_out} layer to highlight fracture features.
        \item \textbf{Confidence Audit}: Implements a three-tier safety logic (Uncertain, Low Confidence, Result Detected).
    \end{itemize}
    \begin{lstlisting}[style=pythonstyle]
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import preprocess_input
import os
import sqlite3
import hashlib
import cv2

# load the models
try:
    model_elbow_frac = tf.keras.models.load_model("weights/ResNet50_Elbow_frac.h5")
    model_hand_frac = tf.keras.models.load_model("weights/ResNet50_Hand_frac.h5")
    model_shoulder_frac = tf.keras.models.load_model("weights/ResNet50_Shoulder_frac.h5")
    model_parts = tf.keras.models.load_model("weights/ResNet50_BodyParts.h5")
except Exception as e:
    print(f"Error loading models: {e}")

# categories for each result
categories_parts = ["Elbow", "Hand", "Shoulder"]
categories_fracture = ['fractured', 'normal']

def make_gradcam_heatmap(img_array, model, last_conv_layer_name="conv5_block3_out", pred_index=None):
    try:
        grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])
    except ValueError:
        return None

    with tf.GradientTape() as tape:
        last_conv_layer_output, preds = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(preds[0])
        class_channel = preds[:, pred_index]

    grads = tape.gradient(class_channel, last_conv_layer_output)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    last_conv_layer_output = last_conv_layer_output[0]
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

def save_gradcam(img_path, heatmap, cam_path):
    if heatmap is None: return None
    img = cv2.imread(img_path)
    if img is None: return None
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    superimposed_img = heatmap * 0.4 + img
    cv2.imwrite(cam_path, superimposed_img)
    return cam_path

def predict(img, model="Parts"):
    size = 224
    image_name = os.path.basename(img) if isinstance(img, str) else str(img)
    temp_img = image.load_img(img, target_size=(size, size))
    x = image.img_to_array(temp_img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)

    if model == 'Parts':
        chosen_model = model_parts
        prediction = np.argmax(chosen_model.predict(x), axis=1)
        prediction_str = categories_parts[prediction.item()]
        return prediction_str
    else:
        if model == 'Elbow': chosen_model = model_elbow_frac
        elif model == 'Hand': chosen_model = model_hand_frac
        elif model == 'Shoulder': chosen_model = model_shoulder_frac
        else: chosen_model = model_parts

        preds = chosen_model.predict(x)
        prob_fracture = preds[0][0]
        
        if prob_fracture > 0.65:
            fracture_detected, confidence_cat, result_title = True, "High", "DETECTED"
            safety_msg = "Model Detected Pattern Consistent With Fracture"
        elif 0.40 <= prob_fracture <= 0.65:
            fracture_detected, confidence_cat, result_title = False, "Moderate", "LOW CONFIDENCE"
            safety_msg = "Low Confidence — Requires Expert Review"
        else:
            fracture_detected, confidence_cat, result_title = False, "Low", "UNCERTAIN"
            safety_msg = "Uncertain — Review Recommended"
            
        heatmap = make_gradcam_heatmap(x, chosen_model, pred_index=0)
        cam_filename = f"cam_{int(prob_fracture*100)}_{image_name}"
        cam_path = os.path.join(os.path.dirname(img), cam_filename)
        save_gradcam(img, heatmap, cam_path)

        return {
            "result": result_title,
            "fracture_detected": fracture_detected,
            "probability": float(prob_fracture),
            "confidence_category": confidence_cat,
            "safety_message": safety_msg,
            "cam_path": cam_path,
            "disclaimer": "Research Prototype - Not a Diagnostic Tool"
        }
    \end{lstlisting}

    \section{Training Pipelines}
    \textbf{Functionality Overview: \texttt{training\_fracture.py}}
    \begin{itemize}
        \item \textbf{Data Ingestion}: Recursively walks dataset structures to map anatomical labels and status.
        \item \textbf{Transfer Learning}: Reconfigures Pre-trained ResNet50 with custom Dense layers (128, 50, 2).
        \item \textbf{Class Balancing}: Automatically computes and applies weights to penalize under-represented fracture samples.
        \item \textbf{Performance Monitoring}: Tracks Recall metrics to ensure clinical sensitivity.
    \end{itemize}
    \begin{lstlisting}[style=pythonstyle]
import numpy as np
import pandas as pd
import os.path
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight
import tensorflow as tf
from tensorflow.keras.optimizers import Adam

def load_path(path, part):
    dataset = []
    for folder in os.listdir(path):
        folder = path + '/' + str(folder)
        if os.path.isdir(folder):
            for body in os.listdir(folder):
                if body == part:
                    body_part = body
                    path_p = folder + '/' + str(body)
                    for id_p in os.listdir(path_p):
                        patient_id = id_p
                        path_id = path_p + '/' + str(id_p)
                        for lab in os.listdir(path_id):
                            if lab.split('_')[-1] == 'positive':
                                label = 'fractured'
                            elif lab.split('_')[-1] == 'negative':
                                label = 'normal'
                            path_l = path_id + '/' + str(lab)
                            for img in os.listdir(path_l):
                                img_path = path_l + '/' + str(img)
                                dataset.append({'body_part': body_part, 'patient_id': patient_id, 'label': label, 'image_path': img_path})
    return dataset

def trainPart(part):
    THIS_FOLDER = os.path.dirname(os.path.abspath(__file__))
    image_dir = THIS_FOLDER + '/Dataset/'
    data = load_path(image_dir, part)
    labels, filepaths = [], []

    for row in data:
        labels.append(row['label'])
        filepaths.append(row['image_path'])

    filepaths = pd.Series(filepaths, name='Filepath').astype(str)
    labels = pd.Series(labels, name='Label')
    images = pd.concat([filepaths, labels], axis=1)

    train_df, test_df = train_test_split(images, train_size=0.9, shuffle=True, random_state=1)

    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(
        preprocessing_function=tf.keras.applications.resnet50.preprocess_input,
        validation_split=0.2, rotation_range=20, zoom_range=0.15,
        width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True, fill_mode='nearest'
    )

    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(
        preprocessing_function=tf.keras.applications.resnet50.preprocess_input)

    train_images = train_generator.flow_from_dataframe(dataframe=train_df, x_col='Filepath', y_col='Label', target_size=(224, 224), batch_size=64, subset='training')
    val_images = train_generator.flow_from_dataframe(dataframe=train_df, x_col='Filepath', y_col='Label', target_size=(224, 224), batch_size=64, subset='validation')
    test_images = test_generator.flow_from_dataframe(dataframe=test_df, x_col='Filepath', y_col='Label', target_size=(224, 224), batch_size=32, shuffle=False)

    pretrained_model = tf.keras.applications.resnet50.ResNet50(input_shape=(224, 224, 3), include_top=False, weights='imagenet', pooling='avg')
    pretrained_model.trainable = False

    x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)
    x = tf.keras.layers.Dense(50, activation='relu')(x)
    outputs = tf.keras.layers.Dense(2, activation='softmax')(x)
    model = tf.keras.Model(pretrained_model.input, outputs)

    unique_classes = np.unique(train_df['Label'])
    weights = class_weight.compute_class_weight('balanced', classes=unique_classes, y=train_df['Label'])
    class_weights_dict = dict(enumerate(weights))

    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.Recall(name='recall')])

    callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
    history = model.fit(train_images, validation_data=val_images, epochs=25, callbacks=[callbacks], class_weight=class_weights_dict)

    model.save(THIS_FOLDER + "/weights/ResNet50_" + part + "_frac.h5")
    plt.plot(history.history['accuracy']); plt.plot(history.history['val_accuracy']); plt.title('model accuracy'); plt.savefig(THIS_FOLDER + "/plots/FractureDetection/" + part + "/_Accuracy.jpeg"); plt.clf()
    plt.plot(history.history['loss']); plt.plot(history.history['val_loss']); plt.title('model loss'); plt.savefig(THIS_FOLDER + "/plots/FractureDetection/" + part + "/_Loss.jpeg"); plt.clf()

categories_parts = ["Elbow", "Hand", "Shoulder"]
for category in categories_parts:
    trainPart(category)
    \end{lstlisting}
    \textbf{Functionality Overview: \texttt{training\_parts.py}}
    \begin{itemize}
        \item \textbf{Anatomical Routing}: Trains a base-level classifier to distinguish between Elbow, Hand, and Shoulder.
        \item \textbf{Model Freezing}: Freezes ResNet50 backbone weights to accelerate feature convergence for multi-class classification.
        \item \textbf{Categorical Handling}: Uses Keras ImageDataGenerator for on-the-fly image batching and validation splitting.
    \end{itemize}
    \begin{lstlisting}[style=pythonstyle]
import numpy as np
import pandas as pd
import os.path
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.optimizers import Adam

def load_path(path):
    dataset = []
    for folder in os.listdir(path):
        folder = path + '/' + str(folder)
        if os.path.isdir(folder):
            for body in os.listdir(folder):
                path_p = folder + '/' + str(body)
                for id_p in os.listdir(path_p):
                    path_id = path_p + '/' + str(id_p)
                    for lab in os.listdir(path_id):
                        path_l = path_id + '/' + str(lab)
                        for img in os.listdir(path_l):
                            img_path = path_l + '/' + str(img)
                            dataset.append({'label': body, 'image_path': img_path})
    return dataset

THIS_FOLDER = os.path.dirname(os.path.abspath(__file__))
image_dir = THIS_FOLDER + '/Dataset'
data = load_path(image_dir)
labels, filepaths = [], []

for row in data:
    labels.append(row['label'])
    filepaths.append(row['image_path'])

images = pd.concat([pd.Series(filepaths, name='Filepath').astype(str), pd.Series(labels, name='Label')], axis=1)
train_df, test_df = train_test_split(images, train_size=0.9, shuffle=True, random_state=1)

train_generator = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=tf.keras.applications.resnet50.preprocess_input, validation_split=0.2)
test_generator = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=tf.keras.applications.resnet50.preprocess_input)

train_images = train_generator.flow_from_dataframe(dataframe=train_df, x_col='Filepath', y_col='Label', target_size=(224, 224), batch_size=64, subset='training')
val_images = train_generator.flow_from_dataframe(dataframe=train_df, x_col='Filepath', y_col='Label', target_size=(224, 224), batch_size=64, subset='validation')
test_images = test_generator.flow_from_dataframe(dataframe=test_df, x_col='Filepath', y_col='Label', target_size=(224, 224), batch_size=32, shuffle=False)

pretrained_model = tf.keras.applications.resnet50.ResNet50(input_shape=(224, 224, 3), include_top=False, weights='imagenet', pooling='avg')
pretrained_model.trainable = False

x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)
x = tf.keras.layers.Dense(50, activation='relu')(x)
outputs = tf.keras.layers.Dense(3, activation='softmax')(x)
model = tf.keras.Model(pretrained_model.input, outputs)

model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = model.fit(train_images, validation_data=val_images, epochs=25, callbacks=[callbacks])

model.save(THIS_FOLDER + "/weights/ResNet50_BodyParts.h5")
plt.plot(history.history['accuracy']); plt.plot(history.history['val_accuracy']); plt.title('model accuracy'); plt.show()
plt.plot(history.history['loss']); plt.plot(history.history['val_loss']); plt.title('model loss'); plt.show()
    \end{lstlisting}
    \section{System Verification}
    \textbf{Functionality Overview: \texttt{verify\_changes.py}}
    \begin{itemize}
        \item \textbf{Automated Unit Testing}: Triggers inference for both Body Part and Fracture models in a single execution.
        \item \textbf{Validation}: Checks return types and dictionary structures for GUI compatibility.
        \item \textbf{Exception Handling}: Implements stack trace logging for debugging weight-loading or image-path errors.
    \end{itemize}
    \begin{lstlisting}[style=pythonstyle]
import sys
import os
import traceback
from predictions import predict

def run_test():
    test_img = "test/Elbow/fractured/ElbowDan.jpeg"
    
    if not os.path.exists(test_img):
        for root, dirs, files in os.walk("test"):
            for file in files:
                if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                    test_img = os.path.join(root, file)
                    break
            if test_img != "test/Elbow/fractured/ElbowDan.jpeg": break
            
    print(f"Testing on {test_img}")
    if not os.path.exists(test_img):
        print("No test image found. Skipping inference test.")
        return

    try:
        print("Predicting Part...")
        part = predict(test_img, "Parts")
        print(f"Part Prediction: {part}")
        
        print(f"Predicting Fracture for {part}...")
        result = predict(test_img, part)
        print(f"Fracture Prediction Result: {result}")
        
        if isinstance(result, dict):
            print("Success: Dictionary returned.")
            print(f"Result: {result['result']}")
            print(f"Probability: {result['probability']}")
            print(f"Cam Path: {result['cam_path']}")
        else:
            print("Failure: Expected dictionary return for fracture prediction.")
            
    except Exception as e:
        print(f"Error during testing: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    run_test()
    \end{lstlisting}
    \end{document}


