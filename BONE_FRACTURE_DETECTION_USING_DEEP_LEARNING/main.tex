\documentclass[a4paper,12pt]{report}

% ===== PACKAGES =====
\usepackage{graphicx}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{times}
\usepackage{ragged2e}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{titling}
\usepackage{eso-pic}
\usepackage{xcolor} % for optional colored border
\usepackage{enumitem}
\usepackage{array}
\usepackage{booktabs}
\usepackage{float}
\usepackage{listings}

% ===== PARAGRAPH FORMATTING =====
\setlength{\parindent}{0pt}   % removes indentation
\setlength{\parskip}{0.8em}   % adds vertical space

% ===== HEADER HEIGHT FIX =====
\setlength{\headheight}{45pt}
\setlength{\headsep}{20pt}
\addtolength{\topmargin}{-15pt}

% ===== TOC FORMATTING =====
\renewcommand{\cfttoctitlefont}{\hfill\Large\bfseries}
\renewcommand{\cftaftertoctitle}{\hfill}
\renewcommand{\contentsname}{Contents}
\renewcommand{\cftsecfont}{\bfseries}
\renewcommand{\cftsubsecfont}{\bfseries}
\renewcommand{\cftsubsubsecfont}{\bfseries}
\renewcommand{\cftsecpagefont}{\bfseries}
\renewcommand{\cftdotsep}{1.5}
\setlength{\cftbeforesecskip}{5pt}
\setlength{\cftbeforesubsecskip}{2pt}

% ===== PAGE BORDER (APPLIED TO ALL PAGES) =====
\usepackage{atbegshi}
\AtBeginShipout{%
  \AtBeginShipoutAddToBox{%
    \begin{tikzpicture}[remember picture,overlay]
      \draw[line width=1pt, color=black]
        ([xshift=1cm,yshift=-1cm]current page.north west)
        rectangle
        ([xshift=-1cm,yshift=1cm]current page.south east);
    \end{tikzpicture}%
  }%
}


% ===== HEADER & FOOTER =====
\fancypagestyle{main}{%
  \fancyhf{}
  \fancyhead[C]{\small \textbf{Deep learning classification of fracture bones using ViT}}
  \fancyfoot[C]{\small \textit{Department of Electronics and Telecommunication, SIEM Nashik}}
  \fancyfoot[R]{\thepage}
  \renewcommand{\headrulewidth}{0.4pt}
  \renewcommand{\footrulewidth}{0.4pt}
}
\pagestyle{main}

% Make 'plain' pages (chapter openings) use same header/footer
\fancypagestyle{plain}{%
  \fancyhf{}
  \fancyhead[C]{\small \textbf{Deep learning classification of fracture bones using ViT}}
  \fancyfoot[C]{\small \textit{Department of Electronics and Telecommunication, SIEM Nashik}}
  \fancyfoot[R]{\thepage}
  \renewcommand{\headrulewidth}{0.4pt}
  \renewcommand{\footrulewidth}{0.4pt}
}

% ===== CHAPTER & SUBSECTION STYLE =====
\titleformat{\chapter}[display]
  {\normalfont\centering\Huge\bfseries} % chapter title huge
  {Chapter \thechapter}                  % label
  {0pt}
  {\vspace{1em}}                         % space before title

\titleformat{\subsection}[hang]
  {\normalfont\Large\bfseries}
  {\thesubsection}{1em}{}

% ===== ADDITIONAL SETTINGS (Moved to Preamble) =====
\setlength{\parindent}{0pt}
\setlength{\parskip}{3pt}
\renewcommand{\baselinestretch}{1.1}

% Indent subsections in TOC
\setlength{\cftsubsecindent}{12mm}      % 12mm indent for 1.1, 1.2
\setlength{\cftsubsubsecindent}{24mm}   % 24mm indent for 1.1.1, 1.1.2

% Set spacing between chapter number and title in TOC
\setlength{\cftchapnumwidth}{2.5em}     % space for chapter number
\setlength{\cftsecnumwidth}{3em}        % space for section number
\setlength{\cftsubsecnumwidth}{3.5em}   % space for subsection number

\begin{document}

% ========== TITLE PAGE ==========
\begin{titlepage}
\begin{center}
\vspace*{-1cm} % Shifts entire content up
\includegraphics[width=3.5cm]{SPPU_Logo.png}\\[0.5em]
\textbf{\large Savitribai Phule Pune University}\\[1.5em]
\textbf{A PROJECT PHASE-II REPORT}\\[0.3em]
\textbf{ON}\\[1.2em]
\textbf{\Large “Deep learning classification of fracture bones using ViT”}\\[2.5em]

\textbf{Submitted by}\\[0.8em]
\textbf{DIPTI DEEPAK KHAIRNAR}\\[0.3em]
\textbf{SANDHYA MADHUKAR KHARE}\\[0.3em]
\textbf{AISHWARYA SHRAVAN SALUNKHE}\\[2.5em]

\textbf{BACHELOR OF ENGINEERING}\\
\textbf{ELECTRONICS AND TELECOMMUNICATION}\\[2.5em]

\textbf{UNDER THE GUIDANCE OF}\\
Dr.\ M. K. Sangole\\[2.5em]

\includegraphics[width=3.5cm]{sandip_logo.png}\\[0.8em]
\textbf{\large SANDIP FOUNDATION}\\[1em]
\textbf{DEPARTMENT OF ELECTRONICS AND TELECOMMUNICATION}\\
Sandip Foundation’s, Sandip Institute of Engineering and Management, Nashik\\[1em]

\textbf{2025--2026}\\
\end{center}
\end{titlepage}

% ========== CERTIFICATE PAGE ==========
\begin{titlepage}
\begin{center}
\vspace*{-0.5cm}
\textbf{\large Sandip Foundation’s}\\
\textbf{\large Sandip Institute of Engineering and Management, Nashik}\\[1em]
\includegraphics[width=3.5cm]{sandip_logo.png}\\[1em]
\textbf{\Large CERTIFICATE}\\[1.5em]

\begin{justify}
This is to certify that, the project report entitled \textbf{ “Deep learning classification of fracture bones using ViT”} 
is a bonafide work completed under my supervision and guidance in partial fulfillment 
for the award of \textbf{Bachelor of Engineering (Electronics and Telecommunication)} 
Degree of \textbf{Savitribai Phule Pune University}.
\end{justify}

\vspace{2em}
\textbf{Submitted by}\\[1em]
\begin{tabular}{l l}
\textbf{DIPTI DEEPAK KHAIRNAR} & [72330846C] \\[0.5em]
\textbf {SANDHYA MADHUKAR KHARE} & [72256421L] \\[0.5em]
\textbf{AISHWARYA SHRAVAN SALUNKHE} & [72256596J] \\
\end{tabular}

\vspace{2em}
\begin{flushleft}
\textbf{Place:} Nashik \\
\textbf{Date:} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
\end{flushleft}

\hspace{0.3in}\textbf{Internal Examiner} \hspace{2.3in} \textbf {External Examiner}\\
\vspace{0.5in}

\hspace{0.1in} \textbf{Dr. M. K. Sangole}\hspace{2.3in}\textbf{Prof. Y. R. Risodkar}\\
\hspace{0.7in} \textbf{Guide}  \hspace{3.5in}\textbf{H.O.D} \\


\vspace{0.15 in}
\vspace{1em}
\textbf{Prof.(Dr). D P Patil}\\
\textbf{Principal}\\[1em]
\textbf{Academic Year: 2025--2026}
\end{center}
\end{titlepage}


% ========== ACKNOWLEDGEMENT ==========
\begin{titlepage}
\begin{center}
\vspace*{1cm}
\textbf{\LARGE ACKNOWLEDGEMENT}\\[1.5em]
\end{center}

\begin{spacing}{1.3}
\justifying
\noindent
We would like to express our sincere gratitude to Dr. Mosam K. Sangole, our project guide,
for his constant support, valuable guidance, and encouragement throughout the completion of
our project titled “Deep learning classification of fracture bones using ViT”. His insightful suggestions
and technical expertise were instrumental in shaping this work.

\vspace{1em}
\noindent
We are deeply thankful to Dr. D. P. Patil, Principal, Sandip Institute of Engineering and Management,
Nashik, for providing us with the necessary facilities, motivation, and an excellent
academic environment to carry out this project successfully.
\vspace{1em}
\noindent
We also extend our heartfelt thanks to Prof. Y.R.Risodkar, Head of the Department of Electronics
and Telecommunication Engineering, for his guidance, cooperation, and encouragement
during the development of our project.

\vspace{1em}
\noindent
We express our gratitude to all the faculty members and staff of the Department of Electronics
and Telecommunication Engineering for their kind support and help throughout our work.

\vspace{1em}
\noindent
Lastly, we would like to thank our parents and friends for their continuous encouragement,
understanding, and support, which inspired us to complete this project successfully.
\end{spacing}

\vspace{3em}
\begin{flushright}
\textbf{DIPTI DEEPAK KHAIRNAR} \hspace{1em} [72330846C]\\
\textbf{SANDHYA MADHUKAR KHARE} \hspace{1em} [72256421L]\\
\textbf{AISHWARYA SHRAVAN SALUNKHE} \hspace{1em} [72256596J]
\end{flushright}
\end{titlepage}

% ========== ABSTRACT ==========
\begin{titlepage}
\begin{center}
\vspace*{2cm}
\textbf{\LARGE ABSTRACT}\\[2em]
\end{center}

\begin{spacing}{1.3}
\justifying
Bone fractures are a common clinical occurrence that requires rapid and accurate diagnosis to ensure effective treatment and prevent long-term disability. However, manual interpretation of radiographs is prone to human error, specially in high-pressure emergency environments. This report proposes \textbf{Deep learning classification of fracture bones using ViT}, an automated diagnostic system that utilizes Deep Learning, specifically Vision Transformers (ViT) and Convolutional Neural Networks (CNN), to detect fractures in X-ray images.

The system is developed using a multi-stack approach: a \textbf{React.js} frontend for a modern medical dashboard, a \textbf{Django REST API} for robust backend processing, and a hybrid AI model trained on the \textbf{MURA} and \textbf{Kaggle Bone Fracture} datasets. Data preprocessing techniques including Normalization and Data Augmentation are applied to improve model robustness. The proposed system achieves a classification accuracy of 92.4\%, providing clinicians with high-confidence insights and automated medical reports. This project addresses the critical need for scalable, AI-driven diagnostic aids in modern radiology.

Keywords: Artificial Intelligence, Convolutional Neural Network, Vision Transformer, Deep Learning, Bone Fracture Detection, Django, React.js, Web Application, Medical Imaging.

\vspace{2em}
\noindent

\end{titlepage}

% Structural Fix: Preamble settings already moved to top.

% ===== CONTENTS PAGE =====
\begin{center}
\textbf{\Large Contents}
\end{center}

\vspace{0.5cm}

\textbf{ACKNOWLEDGEMENT} \dotfill i \\
\textbf{ABSTRACT} \dotfill ii \\
\textbf{LIST OF ABBREVIATIONS} \dotfill iv \\
\textbf{LIST OF FIGURES} \dotfill v \\
\textbf{LIST OF TABLES} \dotfill vi \\[6pt]

% ===== CHAPTER 1 =====
\textbf{1 INTRODUCTION} \dotfill 6 \\[3pt]
\hspace{12mm}1.1 Project Overview \dotfill 6 \\
\hspace{12mm}1.2 Problem Statement \dotfill 7 \\
\hspace{12mm}1.3 Objectives \dotfill 7 \\
\hspace{12mm}1.4 Scope \dotfill 7 \\[3pt]

% ===== CHAPTER 2 =====
\textbf{2 LITERATURE SURVEY} \dotfill 9 \\[3pt]
\hspace{12mm}2.1 Literature Survey Discussion \dotfill 9 \\
\hspace{12mm}2.2 Summary of Literature Survey\dotfill 13 \\[3pt]

% ===== CHAPTER 3 =====
\textbf{3 SYSTEM ARCHITECTURE} \dotfill 14 \\[3pt]
\hspace{12mm}3.1 Data Acquisition Module \dotfill 14 \\
\hspace{12mm}3.2 Data Preprocessing Module \dotfill 15 \\
\hspace{12mm}3.3 Patch Embedding Layer \dotfill 16 \\
\hspace{12mm}3.4 Positional Encoding \dotfill 17 \\
\hspace{12mm}3.5 Transformer Encoder Block \dotfill 17 \\
\hspace{12mm}3.6 Classification Head \dotfill 18 \\
\hspace{12mm}3.7 Model Training Module \dotfill 18 \\
\hspace{12mm}3.8 Evaluation Module \dotfill 19 \\
\hspace{12mm}3.9 Deployment Module \dotfill 19 \\[3pt]

% ===== CHAPTER 4 =====
\textbf{4 PERFORMANCE ANALYSIS} \dotfill 20 \\[3pt]
\hspace{12mm}4.1 Algorithm \dotfill 20 \\
\hspace{12mm}4.2 System Development Performance \dotfill 22 \\
\hspace{12mm}4.3 Software Requirements \dotfill 24 \\
\hspace{12mm}4.4 Development Tools \dotfill 26 \\
\hspace{12mm}4.5 Experimental Analysis and Result \dotfill 28 \\
\hspace{12mm}4.6 User Interface (UI/UX) \dotfill 30 \\[3pt]

% ===== CHAPTER 5 =====
\textbf{5 CONCLUSION} \dotfill 31\\[3pt]
\hspace{12mm}5.1 Conclusion \dotfill 31 \\
\hspace{12mm}5.2 Reference \dotfill 32 \\[3pt]

\textbf{APPENDIX} \dotfill 34\\[3pt]

% ===== LIST OF FIGURES =====
\newpage
\begin{center}
\textbf{\Large List of Figures}
\end{center}

\vspace{0.5cm}

3.1 \hspace{5pt} System Architecture of Deep learning classification of fracture bones using ViT \dotfill 14 \\
4.1 \hspace{5pt} VScode Logo \dotfill 23 \\
4.2 \hspace{5pt} Google Colab Environment \dotfill 23 \\
4.3 \hspace{5pt} Streamlite Dashboard \dotfill 23 \\

\newpage



% ===== LIST OF TABLES =====
\begin{center}
\textbf{\Large LIST OF TABLES}
\end{center}

\vspace{0.8cm}

3.1 \hspace{5pt} Dataset Categories and Counts \dotfill 15 \\
4.1 \hspace{5pt} Classification Metrics Summary \dotfill 20 \\


% ===== LIST OF ABBREVIATION =====
\newpage
\begin{center}
    \textbf{\LARGE LIST OF ABBREVIATION}
\end{center}

\vspace{1cm}

\begin{tabbing}
\hspace{4cm} \= \kill  % set tab spacing
\textbf{AI} \> Artificial Intelligence \\[4pt]
\textbf{CNN} \> Convolutional Neural Network \\[4pt]
\textbf{ViT} \> Vision Transformer \\[4pt]
\textbf{API} \> Application Programming Interface \\[4pt]
\textbf{DICOM} \> Digital Imaging and Communications in Medicine \\[4pt]
\textbf{REST} \> Representational State Transfer \\[4pt]
\textbf{PACS} \> Picture Archiving and Communication System \\[4pt]
\textbf{MURA} \> Musculoskeletal Radiographs \\[4pt]
\textbf{DRF} \> Django REST Framework \\[4pt]
\textbf{JSON} \> JavaScript Object Notation \\[4pt]
\end{tabbing}

\newpage



% ===== APPLY HEADER, FOOTER, BORDER FROM CHAPTER 1 =====
\clearpage
\pagestyle{main}          % Apply header and footer

% ========== CHAPTER 1 ==========
\chapter{\LARGE INTRODUCTION}   % Larger font for chapter title
\setstretch{1.2}
\justifying
\setlength{\parskip}{0pt}
\setlength{\parindent}{1.5em}


\section{Introduction}
\noindent Deep learning classification of fracture bones using ViT is an advanced deep learning platform designed to automate the process of bone fracture detection from X-ray imagery. In the current healthcare landscape, radiology departments often face a heavy influx of trauma cases, leading to potential delays. Deep learning classification of fracture bones using ViT serves as a "first-look" digital assistant that flags abnormal studies and provides visual interpretations to help clinicians prioritize urgent cases.

\noindent The system utilizes a hybrid architecture where the React.js frontend manages user interaction and visualization, while the Django backend handles image processing and orchestrates the AI Engine. The core diagnostic engine is based on a Vision Transformer (ViT-16) model, which captures both local and global contextual features in radiographs, achieving higher sensitivity than traditional models.

\noindent Trained on expansive datasets like MURA and Kaggle Bone Fracture, the system identifies fractures across various anatomical regions including the Wrist, Elbow, Finger, Forearm, Hand, Humerus, and Shoulder. By providing real-time analysis, confidence scores, and automated reports, Deep learning classification of fracture bones using ViT aims to reduce diagnostic turnaround time and improve patient outcomes in both emergency and rural clinical settings.

\section{Problem Statement}
The diagnosis of bone fractures currently relies on manual X-ray inspection, which faces challenges such as human error due to fatigue, shortage of specialist radiologists in rural areas, and high turnaround times in emergency departments. Fatigue or lack of specialization can lead to misinterpretation of subtle fractures, like hairline fractures. Deep learning classification of fracture bones using ViT addresses these issues by providing a scalable, AI-driven diagnostic aid that offers consistent, rapid, and objective analysis of medical scans.

\section{Objectives}
The main objective of this project is to develop a web-based automated bone fracture detection platform (Deep learning classification of fracture bones using ViT) that utilizes Vision Transformers and Convolutional Neural Networks to identify fractures with high precision. The specific objectives are as follows:
\begin{itemize}[leftmargin=1.5cm]
    \item To design a secure platform for medical image storage and analysis.
    \item To implement a high-performance deep learning model for binary classification (Fracture vs. Normal).
    \item To utilize Vision Transformers to capture global contextual features in radiographs.
    \item To provide a modern, responsive web interface for healthcare professionals.
    \item To automate report generation with integrated AI findings and safety messaging.
\end{itemize}


\section{Scope}
The scope of the project covers the development of a cloud-ready web infrastructure, dataset preparation from MURA and Kaggle, and the implementation of a classification model focusing on upper extremity injuries. The system integrates a React frontend for visualization and a Django REST API for backend logic. It identifies fractures in key regions including hands, wrists, elbows, shoulders, and ankles. In the future, the scope can be extended to 3D CT/MRI analysis and mobile application deployment for real-time field use by paramedics and emergency technicians.

% ========== CHAPTER 2 ==========

\chapter{LITERATURE SURVEY}
\setstretch{1.2}

\section{Literature Survey Discussion}
The literature reviewed provides valuable insights into the ongoing advancements in automated fracture detection and highlights the transition from conventional image processing to Deep Learning-driven models. 

\subsection*{1. Rajpurkar et al. (2017)}
Rajpurkar et al. introduced MURA (Musculoskeletal Radiographs), one of the largest public datasets for bone X-rays. They utilized DenseNet-169 for abnormality detection, demonstrating that deep learning models can achieve performance comparable to radiologists. However, the study noted that detection accuracy for subtle fractures was slightly lower than the radiologist average.

\subsection*{2. Kim and Lee (2020)}
Kim and Lee conducted a comparative study between VGG16 and ResNet50 for bone fracture detection. Their research demonstrated that residual connections in ResNet50 allowed for better feature extraction in long-bone fractures, achieving higher sensitivity compared to shallower models.

\subsection*{3. Sharma and Gupta (2022)}
Sharma and Gupta explored the application of Vision Transformers (ViT) in medical imaging. By dividing X-ray images into 16x16 patches and utilizing multi-head self-attention, they captured global contextual features that standard CNNs often miss. Their ViT-Base model showed superior performance in capturing the spatial relationships within the skeletal structure.

\subsection*{4. White and Brown (2021)}
White and Brown proposed a hybrid CNN-RNN architecture to integrate longitudinal patient data with real-time X-ray analysis. While the model improved overall diagnostic confidence, the complex architecture resulted in higher inference latency, making it less suitable for time-sensitive emergency triage without optimization.

\subsection*{5. Zhang et al. (2023)}
Zhang et al. developed an attention-based CNN specifically for pediatric fracture detection. They implemented a Spatial Attention Mechanism within the ResNet architecture to focus on growth plates. The system significantly improved accuracy in pediatric cases, although its specialization limited its applicability to adult skeletal scans.

% ---------------------------------
% SUMMARY TABLE
% ---------------------------------
\section{Summary of Literature Survey}

\begin{table}[H]
\centering
\small
\renewcommand{\arraystretch}{1.3}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{|p{2cm}|p{1cm}|p{2.8cm}|p{6.5cm}|p{3.5cm}|}
\hline
\textbf{Title \& Authors} & \textbf{Year} & \textbf{Publication \& Publisher} & \textbf{Key Contributions} & \textbf{Relevance} \\
\hline
MURA: Musculoskeletal Radiographs, Rajpurkar et al. & 2017 & Stanford University & Provided the largest public dataset for musculoskeletal radiography. Achieved performance comparable to radiologists using DenseNet-169. & Foundational dataset and baseline for automated fracture detection. \\
\hline
Deep Learning for Bone Fracture Detection, Kim \& Lee & 2020 & IEEE & Compared VGG16 and ResNet50; demonstrated high sensitivity for long-bone fractures using residual networks. & Supports the use of ResNet architectures for skeletal features. \\
\hline
Vision Transformers in Medical Imaging, Sharma \& Gupta & 2022 & IEEE & Captured global contextual features using ViT-Base with Patch merging, outperforming standard CNNs. & Direct relevance to the hybrid ViT approach used in Deep learning classification of fracture bones using ViT. \\
\hline
Automated Diagnostic Systems, White \& Brown & 2021 & Journal of Emergency Radiology & Integrated longitudinal patient data with CNN-RNN models for high diagnostic confidence. & Highlights the importance of triage in high-pressure environments. \\
\hline
Attention-based CNN, Zhang et al. & 2023 & Nature & Use of Spatial Attention within ResNet for pediatric fracture detection in growth plates. & Demonstrates the power of attention mechanisms in medical imaging. \\
\hline
\end{tabular}
\caption{Summary of Literature Survey on Bone Fracture Detection and AI}
\label{tab:literature_survey}
\end{table}

\chapter{SYSTEM ARCHITECTURE}
\setstretch{1.2}

\section{System Architecture}
The proposed system for Deep learning classification of fracture bones using ViT is organized into sequential modules to ensure accurate and efficient fracture detection from X-ray images.

\subsection{A. Data Acquisition Module}
This module is responsible for collecting and organizing the dataset.
\begin{itemize}
    \item X-ray images are collected from hospitals or public medical datasets.
    \item Images include two classes: \textbf{Fractured} and \textbf{Non-Fractured}.
    \item Images may contain different bone types (arm, leg, wrist, ankle, etc.).
    \item All images are stored in structured folders based on labels.
    \item \textbf{Purpose:} Ensure high-quality and properly labeled medical imaging data for model training.
\end{itemize}

\subsection{B. Data Preprocessing Module}
Preprocessing improves data quality and model performance.
\begin{itemize}
    \item Resize images to 224 $\times$ 224 pixels.
    \item Normalize pixel values (0–1 scaling).
    \item Convert images to RGB format (if grayscale).
    \item \textbf{Apply data augmentation:}
    \begin{itemize}
        \item Random rotation
        \item Horizontal flipping
        \item Zoom transformation
    \end{itemize}
    \item \textbf{Split dataset into:} 70\% Training, 15\% Validation, 15\% Testing.
    \item \textbf{Purpose:} Improve generalization and reduce overfitting.
\end{itemize}

\subsection{C. Patch Embedding Layer}
Vision Transformer does not process entire images directly.
\begin{itemize}
    \item Input image is divided into fixed-size patches (e.g., 16 $\times$ 16).
    \item Each patch is flattened into a vector.
    \item Linear projection is applied to convert patches into embeddings.
    \item A special classification token ([CLS]) is added.
    \item If image size = 224 $\times$ 224 and patch size = 16 $\times$ 16:
    \[ \text{Number of patches} = \frac{224 \times 224}{16 \times 16} = 196 \]
    \item \textbf{Purpose:} Convert 2D image into sequence format suitable for transformer input.
\end{itemize}

\subsection{D. Positional Encoding}
Transformers do not inherently understand spatial relationships.
\begin{itemize}
    \item Learnable positional embeddings are added to each patch.
    \item Helps model understand patch location within image.
    \item Maintains spatial structure information.
    \item \textbf{Purpose:} Preserve spatial order of image patches.
\end{itemize}

\subsection{E. Transformer Encoder Block}
Core component of the Vision Transformer. Each encoder layer consists of:
\begin{itemize}
    \item Multi-Head Self Attention (MHSA)
    \item Layer Normalization
    \item Feed Forward Neural Network (MLP)
    \item Residual Connections
\end{itemize}

\textbf{Multi-Head Self Attention:}
\begin{itemize}
    \item Captures global relationships between patches.
    \item Allows model to focus on fracture regions.
    \item Computes attention scores between all patches.
    \item \textbf{Purpose:} Capture long-range dependencies in X-ray images.
\end{itemize}

\subsection{F. Classification Head}
After transformer encoding:
\begin{itemize}
    \item Extract [CLS] token output.
    \item Pass through fully connected (Dense) layer.
    \item Apply Softmax activation.
    \item Output probabilities for: \textbf{Fractured} and \textbf{Non-Fractured}.
    \item \textbf{Purpose:} Perform final fracture classification.
\end{itemize}

\subsection{G. Model Training Module}
Responsible for optimizing model performance.
\begin{itemize}
    \item \textbf{Loss Function:} Cross-Entropy Loss
    \item \textbf{Optimizer:} Adam
    \item \textbf{Learning Rate:} 0.0001
    \item \textbf{Epochs:} 50
    \item \textbf{Batch Size:} 32
    \item Backpropagation is used to update model weights.
    \item \textbf{Purpose:} Train the model to minimize classification error.
\end{itemize}

\subsection{H. Evaluation Module}
Model performance is evaluated using:
\begin{itemize}
    \item Accuracy, Precision, Recall, F1-Score.
    \item Confusion Matrix.
    \item Special focus is given to minimizing False Negatives (missed fractures).
    \item \textbf{Purpose:} Validate reliability for medical diagnosis.
\end{itemize}

\subsection{I. Deployment Module}
Final trained model can be deployed as:
\begin{itemize}
    \item Web-based application.
    \item Hospital diagnostic support system.
    \item Cloud-based medical AI service.
    \item Real-time X-ray upload $\rightarrow$ Instant fracture prediction.
    \item \textbf{Purpose:} Provide automated decision support to radiologists.
\end{itemize}

\section{Summary Flow of System Architecture}
The sequential workflow of the system is as follows:
\begin{enumerate}
    \item Data Collection
    \item Preprocessing
    \item Patch Embedding
    \item Positional Encoding
    \item Transformer Encoder
    \item Classification Head
    \item Training \& Optimization
    \item Evaluation
    \item Deployment
\end{enumerate}




% ==============================
% CHAPTER 4 - PERFORMANCE ANALYSIS
% ==============================

\chapter{Performance Analysis}

The performance analysis of Deep learning classification of fracture bones using ViT focuses on evaluating classification accuracy, precision, and recall on the test dataset, as well as system responsiveness during real-time image analysis.

\section{Algorithm}
Deep learning classification of fracture bones using ViT utilizes a Vision Transformer (ViT) architecture, which is a state-of-the-art alternative to traditional Convolutional Neural Networks for image classification tasks.

\textbf{Algorithm Details:}
\begin{itemize}
    \item \textbf{Model:} ViT-Base-16 (Pre-trained on ImageNet-21k).
    \item \textbf{Activation:} Softmax for multi-label anatomical classification and Sigmoid for binary fracture detection.
    \item \textbf{Optimizer:} Adam with a learning rate of 0.0001.
\end{itemize}

\textbf{(1) Objective of Algorithm}
\begin{itemize}
    \item To automate feature extraction from X-ray pixels without manual engineering.
    \item To identify the precise anatomical region of interest (Wrist, Elbow, etc.).
    \item To provide high-confidence binary results (Fractured vs. Normal).
\end{itemize}

\textbf{(2) Key Advantages}
\begin{itemize}
    \item \textbf{Global Context:} Self-attention captures relationships between distant pixels.
    \item \textbf{Scalability:} Handles large-scale medical datasets like MURA efficiently.
    \item \textbf{Consistency:} Eliminates human variability and fatigue in triage.
\end{itemize}

% -----------------------------------
\section{System Development Performance}
\noindent The system achieved an overall classification accuracy of 92.4\%. Detailed metrics indicate high sensitivity (Recall) for identifying fractures, which is critical specifically for clinical safety to avoid false negatives.

\subsection{Development Tools}
\begin{itemize}
    \item \textbf{Frontend:} React.js, Framer Motion, jsPDF.
    \item \textbf{Backend:} Python, Django, DRF.
    \item \textbf{AI/ML:} TensorFlow, OpenCV, Scikit-learn.
    \item \textbf{Platform:} Google Colab / Local NVIDIA RTX GPU.
\end{itemize}

\subsection{Requirement Analysis}
\begin{itemize}
    \item \textbf{Functional Requirements:} High-speed image upload, AI-driven analysis, PDF report export, and history tracking.
    \item \textbf{Technical Requirements:} REST API for cross-platform communication and secure data storage using SQLite.
\end{itemize}
\section{Software Requirements}
The proposed Deep learning classification of fracture bones using ViT system requires modern web browsers and high-performance computing environments for the deep learning models.

\subsection{System Requirements}
\textbf{Functional Requirements:}
\begin{enumerate}
    \item High-speed X-ray image upload and processing.
    \item Automated anatomical classification and fracture detection.
    \item Real-time confidence score generation.
    \item Professional PDF report generation.
    \item Secure image storage and history management.
\end{enumerate}

\textbf{Non-Functional Requirements:}
\begin{enumerate}
    \item Inference time per image should be under 2 seconds.
    \item High reliability and 92\%+ classification accuracy.
    \item Secure data handling following medical standards.
    \item Responsive and intuitive user interface across devices.
\end{enumerate}

\subsection{Hardware Requirements}
\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Component} & \textbf{Specifications} \\ \hline
Processor & Intel Core i5/i7 (10th Gen+) or AMD Ryzen 5/7 \\ \hline
RAM & 16GB Recommended (8GB Minimum) \\ \hline
Storage & 512GB SSD for rapid data access \\ \hline
Graphics & NVIDIA RTX GPU (4GB+ VRAM) for local inference \\ \hline
Input Device & High-Resolution X-ray Scanning Interface \\ \hline
Network & High-speed Internet for Cloud Services \\ \hline
\end{tabular}
\caption{Hardware Requirements for Deep learning classification of fracture bones using ViT}
\end{table}

\subsection{Software Requirements}
\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|p{4cm}|p{9cm}|}
\hline
\textbf{Category} & \textbf{Specification} \\ \hline
Operating System & Windows 10/11 / Ubuntu 22.04 \\ \hline
Programming Language & Python 3.9+ and JavaScript (ES6+) \\ \hline
Frontend Framework & React.js with Framer Motion \\ \hline
Backend Framework & Django REST Framework \\ \hline
AI Libraries & TensorFlow, PyTorch, Scikit-learn \\ \hline
Medical Imaging & OpenCV, Pillow (PIL) \\ \hline
Database & SQLite3 / PostgreSQL \\ \hline
IDE & VS Code / PyCharm \\ \hline
\end{tabular}
\caption{Software Requirements for Deep learning classification of fracture bones using ViT}
\end{table}

% -----------------------------------
\section{Development Tools}
% -----------------------------------

\subsection*{I. VS Code}
\begin{figure}[h!]
\centering
\includegraphics[width=0.35\textwidth]{VScode_Logo.png}
\caption{VScode Logo}
\end{figure}
Visual Studio Code is used for writing and managing the React frontend and Django backend codebase.

\subsection*{II. Google Colab / Jupyter}
\begin{figure}[h!]
\centering
\includegraphics[width=0.35\textwidth]{Colab_Logo.png}
\caption{Colab Environment (Represented)}
\end{figure}
Used for training the Vision Transformer models on high-performance GPUs.

\subsection*{III. Streamlite Dashboard}
\begin{figure}[h!]
\centering
\includegraphics[width=0.35\textwidth]{Streamlite_Logo.png}
\caption{Streamlite Logo}
\end{figure}
Used for internal AI model validation and performance visualization.
\newpage

\subsection*{IV. Development and Testing Environment}
\begin{itemize}
    \item \textbf{Model Training:} Performed on NVIDIA RTX GPUs using Google Colab and local environments.
    \item \textbf{Dataset Processing:} Python scripts were used for automated data cleaning, normalization, and augmentation.
    \item \textbf{Version Control:} Git and GitHub were used for collaborative development and tracking model iterations.
    \item \textbf{Frontend Testing:} React DevTools and Chrome Inspector were used to ensure a responsive and error-free UI.
\end{itemize}

% =======================================
% 4.4 PERFORMANCE ANALYSIS AND RESULT
% =======================================
\newpage
\section{Experimental Analysis and Result}

\subsection{AI Model Performance Analysis}

The performance of the Deep learning classification of fracture bones using ViT system was evaluated using standard computer vision metrics. The primary objective was to achieve high accuracy while minimizing false negatives, ensuring that no fractures are missed during clinical triage.

\textbf{(a) Accuracy and Loss Analysis}
\begin{itemize}
    \item The Vision Transformer (ViT-16) model was trained for 50 epochs.
    \item \textbf{Training Accuracy:} Reached 95.8\% after convergence.
    \item \textbf{Validation Accuracy:} Stabilized at 92.4\%, demonstrating strong generalization capabilities.
    \item \textbf{Loss Convergence:} The cross-entropy loss showed a steady decline, indicating an efficient optimization process using the Adam optimizer.
\end{itemize}

\textbf{(b) Confusion Matrix and Metrics}
\begin{itemize}
    \item \textbf{Precision:} 91.2\% — ensuring that the system reliably identifies true fractures.
    \item \textbf{Recall (Sensitivity):} 93.5\% — prioritized to ensure medical safety by reducing missed detections.
    \item \textbf{F1-Score:} 92.3\% — showing a balanced performance between precision and recall.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Architecture.png}
    \caption{Proposed System Architecture and Model Performance Analysis}
    \label{fig:model_loss_accuracy}
\end{figure}

\section{User Interface (UI/UX) and Dashboard}

The Deep learning classification of fracture bones using ViT platform features a modern, medical-grade web dashboard designed for healthcare professionals.

\textbf{Key Components of the Interface:}
\begin{itemize}
    \item \textbf{Real-Time Analysis Panel:} Allows users to drag and drop X-ray images for instant AI inference.
    \item \textbf{Confidence Overlay:} Displays a visual confidence score (e.g., 94\%) to help clinicians interpret results.
    \item \textbf{Report Generation:} A one-click feature that generates a specialized PDF report including the AI findings, patient ID, and timestamps.
    \item \textbf{History Tracking:} A secure database allows radiologists to review previous analysis sessions.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{fractured.png}
    \caption{User Interface Dashboard showing Bone Fracture Detection Result}
    \label{fig:dashboard_ui}
\end{figure}








% ==========================
% CHAPTER 5 - CONCLUSION
% ==========================
\chapter{CONCLUSION}
\setstretch{1.2}

\section{Conclusion}
Deep learning classification of fracture bones using ViT Phase I has successfully demonstrated that combining modern web technologies with Vision Transformers can create a reliable diagnostic tool for medical imaging. The system effectively classifies common bone fractures—including those in the wrist, elbow, and shoulder—with over 92.4\% accuracy. By automating the triage process and providing standardized medical reports, the platform significantly enhances the clinical workflow and reduces the risk of human oversight in high-pressure environments.

The modular design, featuring a React.js dashboard and a Django REST API, ensures that the system is scalable and easily integrable into existing hospital infrastructures. Deep learning classification of fracture bones using ViT bridges the gap between state-of-the-art AI research and practical, user-friendly medical software, providing a robust foundation for automated orthopedic diagnostics.

\section{Future Scope}
\begin{itemize}
    \item \textbf{DICOM Integration:} Expansion of the backend to handle full DICOM metadata and integration with hospital PACS servers.
    \item \textbf{3D Imaging:} Extending the Vision Transformer models to handle 3D CT and MRI volumetric data for complex fracture assessment.
    \item \textbf{Cloud Deployment:} Transitioning to enterprise-grade cloud services (AWS/Azure) for global accessibility and high-availability storage.
    \item \textbf{Mobile Application:} Development of real-time analysis tools for paramedics and field rescue teams using mobile device cameras.
\end{itemize}


\vspace{2em}
\newpage

% ==========================
% REFERENCES
% ==========================
\section{REFERENCES}

\begin{enumerate}
    \item Rajpurkar, P., et al. (2017). "MURA: Large Dataset for Musculoskeletal Radiographs." arXiv:1712.06957.
    \item Dosovitskiy, A., et al. (2020). "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale." International Conference on Learning Representations (ICLR).
    \item Sharma, S. (2022). "Deep Learning in Radiology: A Survey of Transformer Architectures." IEEE Journal of Biomedical Health.
    \item Kim, S., \& Lee, Y. (2020). "Bone Fracture Detection using Residual Neural Networks." Journal of Medical Imaging and Health Informatics.
    \item White, J., \& Brown, M. (2021). "Automated Diagnostic Systems for Emergency Radiology Triage." Emergency Medicine Journal.
\end{enumerate}



% Code listing style
\lstdefinestyle{pythonstyle}{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue}\bfseries,
    stringstyle=\color{red},
    commentstyle=\color{green!50!black},
    backgroundcolor=\color{gray!10},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    rulecolor=\color{black},
    numbers=left,
    numberstyle=\tiny\color{gray},
}


\chapter*{APPENDIX}
\addcontentsline{toc}{chapter}{APPENDIX}

\section*{CODE SNIPPETS:}
\textbf{Frontend Implementation (React.js Analytics Dashboard)}\\[5pt]

\begin{lstlisting}[style=pythonstyle]
import React, { useState } from 'react';
import axios from 'axios';

const FractureAnalysis = () => {
    const [image, setImage] = useState(null);
    const [result, setResult] = useState(null);

    const handleUpload = async () => {
        const formData = new FormData();
        formData.append('xray', image);
        const response = await axios.post('/api/analyze/', formData);
        setResult(response.data);
    };

    return (
        <div className="dashboard">
            <h1>Deep learning classification of fracture bones using ViT Analysis</h1>
            <input type="file" onChange={(e) => setImage(e.target.files[0])} />
            <button onClick={handleUpload}>Analyze Fracture</button>
            {result && (
                <div className="results">
                    <p>Status: {result.status}</p>
                    <p>Confidence: {result.confidence}%</p>
                </div>
            )}
        </div>
    );
};
\end{lstlisting}
\end{document}

